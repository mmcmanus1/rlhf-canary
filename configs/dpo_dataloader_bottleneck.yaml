name: dpo_dataloader_bottleneck
description: Simulates dataloader bottleneck via small batch

# Model configuration
model_name: EleutherAI/pythia-70m
use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Training configuration - SMALL BATCH = MORE DATALOADER CALLS
training_type: dpo
max_steps: 60
batch_size: 1              # Tiny batch = high Python overhead
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
max_length: 256
warmup_steps: 5

# DPO-specific
beta: 0.1
max_prompt_length: 64

# Dataset configuration
dataset_name: Anthropic/hh-rlhf
dataset_split: train
dataset_size: 256
seed: 42

output_dir: ./rca_output
metrics_warmup_steps: 5

profiler:
  enabled: false
