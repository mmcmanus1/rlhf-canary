name: dpo_unstable_lr
description: Intentionally unstable config - high learning rate

# Model configuration
model_name: EleutherAI/pythia-70m
use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# Training configuration - DANGEROUSLY HIGH LR
training_type: dpo
max_steps: 50
batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 1.0e-2   # 200x higher than normal!
max_length: 256
warmup_steps: 5

# DPO-specific
beta: 0.1
max_prompt_length: 64

# Dataset configuration
dataset_name: Anthropic/hh-rlhf
dataset_split: train
dataset_size: 256
seed: 42

# Output configuration
output_dir: ./stability_output
metrics_warmup_steps: 5

profiler:
  enabled: false
