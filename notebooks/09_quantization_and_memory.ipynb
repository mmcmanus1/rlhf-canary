{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mmcmanus1/rlhf-canary/blob/main/notebooks/09_quantization_and_memory.ipynb)\n",
    "\n",
    "# Quantization & Memory Optimization: Running Canaries on Limited Hardware\n",
    "\n",
    "Use 4-bit and 8-bit quantization to run canaries on memory-constrained GPUs. Learn baseline management workflows and compare quantized vs full-precision training.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Enabling 4-bit quantization with bitsandbytes\n",
    "2. Enabling 8-bit quantization\n",
    "3. Memory savings: quantized vs full precision\n",
    "4. Performance impact of quantization\n",
    "5. Baseline management workflows\n",
    "6. Comparing baselines across configurations\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime > Change runtime type > T4 GPU)\n",
    "\n",
    "**Runtime:** ~20-25 minutes (multiple runs for comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport sys\n\nprint(\"Starting Environment Setup...\")\n\n# --- 1. Clone or update the repo ---\nif not os.path.exists(\"/content/rlhf-canary\"):\n    !git clone https://github.com/mmcmanus1/rlhf-canary.git /content/rlhf-canary\nelse:\n    !cd /content/rlhf-canary && git pull --ff-only\n\n%cd /content/rlhf-canary\n\n# --- 2. Force-Install the \"Safe Harbor\" Stack ---\n!pip install \"trl==0.11.4\" \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"tokenizers==0.19.1\" --force-reinstall --no-deps --quiet\n!pip install -q datasets pydantic click PyYAML bitsandbytes\nprint(\"Libraries installed (TRL 0.11.4 / Transformers 4.44.2)\")\n\n# --- 3. Patch pyproject.toml ---\nproject_file = \"/content/rlhf-canary/pyproject.toml\"\nif os.path.exists(project_file):\n    with open(project_file, \"r\") as f:\n        content = f.read()\n    \n    if \"trl==0.11.4\" not in content:\n        content = re.sub(r'trl[<>=!~]+[\\d\\.]+', 'trl==0.11.4', content)\n        with open(project_file, \"w\") as f:\n            f.write(content)\n        print(\"Config file patched to lock TRL 0.11.4\")\n\n# --- 4. Patch Source Code ---\nrunner_file = \"/content/rlhf-canary/canary/runner/local.py\"\nif os.path.exists(runner_file):\n    with open(runner_file, \"r\") as f:\n        code = f.read()\n    \n    if \"processing_class=\" in code:\n        code = code.replace(\"processing_class=\", \"tokenizer=\")\n        with open(runner_file, \"w\") as f:\n            f.write(code)\n        print(\"Code patched: Reverted 'processing_class' to 'tokenizer'\")\n    else:\n        print(\"Code is already compatible.\")\n\n# --- 5. Install the package ---\n!pip install -e . --quiet\n\nprint(\"Environment Ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Check if bitsandbytes is working\n",
    "    try:\n",
    "        import bitsandbytes\n",
    "        print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"WARNING: bitsandbytes not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Quantization Options\n",
    "\n",
    "RLHF Canary supports two quantization modes via bitsandbytes:\n",
    "\n",
    "| Option | Memory Savings | Speed Impact | Use Case |\n",
    "|--------|---------------|--------------|----------|\n",
    "| `load_in_4bit` | ~75% | Slight slowdown | Very limited VRAM (<8GB) |\n",
    "| `load_in_8bit` | ~50% | Minimal | Moderate VRAM (8-12GB) |\n",
    "| None (FP16) | Baseline | Fastest | Ample VRAM (16GB+) |\n",
    "\n",
    "### How It Works\n",
    "\n",
    "When `load_in_4bit: true` is set, RLHF Canary uses:\n",
    "\n",
    "```python\n",
    "BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,  # Extra compression\n",
    "    bnb_4bit_quant_type=\"nf4\",       # Normalized float 4-bit\n",
    ")\n",
    "```\n",
    "\n",
    "For 8-bit:\n",
    "```python\n",
    "BitsAndBytesConfig(load_in_8bit=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Quantized Configurations\n",
    "\n",
    "Let's create three configs: full precision (FP16), 8-bit, and 4-bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full precision config (baseline)\n",
    "fp16_config = \"\"\"\n",
    "name: dpo_fp16\n",
    "description: DPO canary with full precision (FP16)\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# No quantization - full FP16\n",
    "load_in_4bit: false\n",
    "load_in_8bit: false\n",
    "\n",
    "training_type: dpo\n",
    "max_steps: 50\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 5\n",
    "\n",
    "beta: 0.1\n",
    "max_prompt_length: 64\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 512\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./canary_output\n",
    "metrics_warmup_steps: 5\n",
    "\"\"\"\n",
    "\n",
    "!mkdir -p configs/quantization\n",
    "with open('configs/quantization/dpo_fp16.yaml', 'w') as f:\n",
    "    f.write(fp16_config)\n",
    "\n",
    "print(\"Created configs/quantization/dpo_fp16.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-bit quantization config\n",
    "int8_config = \"\"\"\n",
    "name: dpo_8bit\n",
    "description: DPO canary with 8-bit quantization\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# 8-bit quantization\n",
    "load_in_4bit: false\n",
    "load_in_8bit: true\n",
    "\n",
    "training_type: dpo\n",
    "max_steps: 50\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 5\n",
    "\n",
    "beta: 0.1\n",
    "max_prompt_length: 64\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 512\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./canary_output\n",
    "metrics_warmup_steps: 5\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/quantization/dpo_8bit.yaml', 'w') as f:\n",
    "    f.write(int8_config)\n",
    "\n",
    "print(\"Created configs/quantization/dpo_8bit.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config\n",
    "int4_config = \"\"\"\n",
    "name: dpo_4bit\n",
    "description: DPO canary with 4-bit quantization (NF4)\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# 4-bit quantization (NF4 with double quantization)\n",
    "load_in_4bit: true\n",
    "load_in_8bit: false\n",
    "\n",
    "training_type: dpo\n",
    "max_steps: 50\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 5\n",
    "\n",
    "beta: 0.1\n",
    "max_prompt_length: 64\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 512\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./canary_output\n",
    "metrics_warmup_steps: 5\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/quantization/dpo_4bit.yaml', 'w') as f:\n",
    "    f.write(int4_config)\n",
    "\n",
    "print(\"Created configs/quantization/dpo_4bit.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all configs\n",
    "!ls -la configs/quantization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Full Precision Baseline\n",
    "\n",
    "First, let's establish a full precision baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full precision canary\n",
    "!python -m canary.cli run configs/quantization/dpo_fp16.yaml -o ./canary_output/fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display FP16 metrics\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "fp16_path = next(Path('./canary_output/fp16').rglob('metrics.json'))\n",
    "\n",
    "with open(fp16_path) as f:\n",
    "    fp16_metrics = json.load(f)\n",
    "\n",
    "print(\"FP16 (Full Precision) Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Step time (mean): {fp16_metrics['perf']['step_time']['mean']:.4f}s\")\n",
    "print(f\"Tokens/sec: {fp16_metrics['perf']['approx_tokens_per_sec']:.0f}\")\n",
    "print(f\"Peak memory: {fp16_metrics['perf']['max_mem_mb']:.0f} MB\")\n",
    "print(f\"NaN steps: {fp16_metrics['stability']['nan_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running 8-bit Quantized Canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 8-bit quantized canary\n",
    "!python -m canary.cli run configs/quantization/dpo_8bit.yaml -o ./canary_output/8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display 8-bit metrics\n",
    "int8_path = next(Path('./canary_output/8bit').rglob('metrics.json'))\n",
    "\n",
    "with open(int8_path) as f:\n",
    "    int8_metrics = json.load(f)\n",
    "\n",
    "print(\"8-bit Quantization Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Step time (mean): {int8_metrics['perf']['step_time']['mean']:.4f}s\")\n",
    "print(f\"Tokens/sec: {int8_metrics['perf']['approx_tokens_per_sec']:.0f}\")\n",
    "print(f\"Peak memory: {int8_metrics['perf']['max_mem_mb']:.0f} MB\")\n",
    "print(f\"NaN steps: {int8_metrics['stability']['nan_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running 4-bit Quantized Canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 4-bit quantized canary\n",
    "!python -m canary.cli run configs/quantization/dpo_4bit.yaml -o ./canary_output/4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display 4-bit metrics\n",
    "int4_path = next(Path('./canary_output/4bit').rglob('metrics.json'))\n",
    "\n",
    "with open(int4_path) as f:\n",
    "    int4_metrics = json.load(f)\n",
    "\n",
    "print(\"4-bit Quantization Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Step time (mean): {int4_metrics['perf']['step_time']['mean']:.4f}s\")\n",
    "print(f\"Tokens/sec: {int4_metrics['perf']['approx_tokens_per_sec']:.0f}\")\n",
    "print(f\"Peak memory: {int4_metrics['perf']['max_mem_mb']:.0f} MB\")\n",
    "print(f\"NaN steps: {int4_metrics['stability']['nan_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"Memory & Performance Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<25} {'FP16':>12} {'8-bit':>12} {'4-bit':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Memory\n",
    "fp16_mem = fp16_metrics['perf']['max_mem_mb']\n",
    "int8_mem = int8_metrics['perf']['max_mem_mb']\n",
    "int4_mem = int4_metrics['perf']['max_mem_mb']\n",
    "\n",
    "print(f\"{'Peak Memory (MB)':<25} {fp16_mem:>12.0f} {int8_mem:>12.0f} {int4_mem:>12.0f}\")\n",
    "print(f\"{'Memory Savings':<25} {'baseline':>12} {(1 - int8_mem/fp16_mem)*100:>11.1f}% {(1 - int4_mem/fp16_mem)*100:>11.1f}%\")\n",
    "\n",
    "# Step time\n",
    "fp16_time = fp16_metrics['perf']['step_time']['mean']\n",
    "int8_time = int8_metrics['perf']['step_time']['mean']\n",
    "int4_time = int4_metrics['perf']['step_time']['mean']\n",
    "\n",
    "print(f\"{'Step Time (s)':<25} {fp16_time:>12.4f} {int8_time:>12.4f} {int4_time:>12.4f}\")\n",
    "print(f\"{'Slowdown':<25} {'baseline':>12} {(int8_time/fp16_time - 1)*100:>+11.1f}% {(int4_time/fp16_time - 1)*100:>+11.1f}%\")\n",
    "\n",
    "# Throughput\n",
    "fp16_tps = fp16_metrics['perf']['approx_tokens_per_sec']\n",
    "int8_tps = int8_metrics['perf']['approx_tokens_per_sec']\n",
    "int4_tps = int4_metrics['perf']['approx_tokens_per_sec']\n",
    "\n",
    "print(f\"{'Tokens/sec':<25} {fp16_tps:>12.0f} {int8_tps:>12.0f} {int4_tps:>12.0f}\")\n",
    "print(f\"{'TPS Change':<25} {'baseline':>12} {(int8_tps/fp16_tps - 1)*100:>+11.1f}% {(int4_tps/fp16_tps - 1)*100:>+11.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison (text-based chart)\n",
    "def text_bar(value, max_val, width=40, label=\"\"):\n",
    "    filled = int((value / max_val) * width)\n",
    "    bar = \"|\" + \"=\" * filled + \" \" * (width - filled) + \"|\"\n",
    "    return f\"{label:<10} {bar} {value:.0f}\"\n",
    "\n",
    "max_mem = max(fp16_mem, int8_mem, int4_mem)\n",
    "\n",
    "print(\"\\nMemory Usage Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(text_bar(fp16_mem, max_mem, label=\"FP16\"))\n",
    "print(text_bar(int8_mem, max_mem, label=\"8-bit\"))\n",
    "print(text_bar(int4_mem, max_mem, label=\"4-bit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Mode\n",
    "\n",
    "| GPU Memory | Recommended | Notes |\n",
    "|------------|-------------|-------|\n",
    "| 4-6 GB | 4-bit | Essential for small GPUs |\n",
    "| 8-12 GB | 8-bit | Good balance |\n",
    "| 16+ GB | FP16 | Best quality and speed |\n",
    "\n",
    "**Trade-offs:**\n",
    "- **4-bit**: Significant memory savings, slight quality/speed impact\n",
    "- **8-bit**: Moderate savings, minimal quality impact\n",
    "- **FP16**: Baseline quality and speed, requires more memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Baseline Management Workflows\n",
    "\n",
    "Proper baseline management is crucial for meaningful regression detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline directory structure\n",
    "!mkdir -p baselines/t4/fp16\n",
    "!mkdir -p baselines/t4/8bit\n",
    "!mkdir -p baselines/t4/4bit\n",
    "\n",
    "print(\"Baseline directory structure:\")\n",
    "!find baselines -type d | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baselines using canary save-baseline command\n",
    "!python -m canary.cli save-baseline {fp16_path} baselines/t4/fp16/dpo_smoke.json\n",
    "!python -m canary.cli save-baseline {int8_path} baselines/t4/8bit/dpo_smoke.json\n",
    "!python -m canary.cli save-baseline {int4_path} baselines/t4/4bit/dpo_smoke.json\n",
    "\n",
    "print(\"\\nSaved baselines:\")\n",
    "!find baselines -name \"*.json\" | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Baseline Organization\n",
    "\n",
    "```\n",
    "baselines/\n",
    "├── t4/                    # GPU type\n",
    "│   ├── fp16/\n",
    "│   │   ├── dpo_smoke.json\n",
    "│   │   └── ppo_smoke.json\n",
    "│   ├── 8bit/\n",
    "│   │   └── dpo_smoke.json\n",
    "│   └── 4bit/\n",
    "│       └── dpo_smoke.json\n",
    "├── a100/                  # Different GPU\n",
    "│   └── fp16/\n",
    "│       └── dpo_smoke.json\n",
    "└── README.md              # Document baselines\n",
    "```\n",
    "\n",
    "**Key principles:**\n",
    "1. **Separate by GPU** - Performance varies significantly\n",
    "2. **Separate by quantization** - Different memory/speed profiles\n",
    "3. **Version control baselines** - Track changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document baselines\n",
    "readme = \"\"\"\n",
    "# Baselines\n",
    "\n",
    "## T4 GPU (16GB VRAM)\n",
    "\n",
    "| Config | Quantization | Memory | Step Time | Created |\n",
    "|--------|--------------|--------|-----------|----------|\n",
    "| dpo_smoke | FP16 | {fp16_mem:.0f}MB | {fp16_time:.4f}s | {date} |\n",
    "| dpo_smoke | 8-bit | {int8_mem:.0f}MB | {int8_time:.4f}s | {date} |\n",
    "| dpo_smoke | 4-bit | {int4_mem:.0f}MB | {int4_time:.4f}s | {date} |\n",
    "\n",
    "## Usage\n",
    "\n",
    "Compare to appropriate baseline based on your hardware and quantization:\n",
    "\n",
    "```bash\n",
    "# FP16 comparison\n",
    "canary compare current.json baselines/t4/fp16/dpo_smoke.json\n",
    "\n",
    "# 4-bit comparison\n",
    "canary compare current.json baselines/t4/4bit/dpo_smoke.json\n",
    "```\n",
    "\"\"\".format(\n",
    "    fp16_mem=fp16_mem, fp16_time=fp16_time,\n",
    "    int8_mem=int8_mem, int8_time=int8_time,\n",
    "    int4_mem=int4_mem, int4_time=int4_time,\n",
    "    date=\"2024-01-01\"  # Replace with actual date\n",
    ")\n",
    "\n",
    "with open('baselines/README.md', 'w') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(\"Created baselines/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Configuration Comparison\n",
    "\n",
    "When comparing runs with different quantization levels, be aware of expected differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 4-bit run to FP16 baseline (expect differences!)\n",
    "print(\"Comparing 4-bit run to FP16 baseline\")\n",
    "print(\"(This will likely show 'regressions' due to quantization overhead)\\n\")\n",
    "\n",
    "!python -m canary.cli compare {int4_path} baselines/t4/fp16/dpo_smoke.json --threshold-tier smoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 4-bit run to 4-bit baseline (apples to apples)\n",
    "print(\"Comparing 4-bit run to 4-bit baseline\")\n",
    "print(\"(This is the correct comparison for regression detection)\\n\")\n",
    "\n",
    "!python -m canary.cli compare {int4_path} baselines/t4/4bit/dpo_smoke.json --threshold-tier smoke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Configuration Comparison Guidelines\n",
    "\n",
    "| Comparison | Valid? | Notes |\n",
    "|------------|--------|-------|\n",
    "| FP16 vs FP16 | Yes | Ideal for regression detection |\n",
    "| 4-bit vs 4-bit | Yes | Apples to apples |\n",
    "| 4-bit vs FP16 | No | Different performance profiles |\n",
    "| T4 vs A100 | No | Hardware too different |\n",
    "\n",
    "**Best practice:** Always compare like-to-like. Create separate baselines for each configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practical Workflow: Memory-Constrained CI\n",
    "\n",
    "Here's how to set up canary testing on limited hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory-optimized config for CI\n",
    "ci_config = \"\"\"\n",
    "name: dpo_ci_4bit\n",
    "description: Memory-optimized DPO canary for CI with limited GPU memory\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 8           # Smaller LoRA rank to save memory\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# 4-bit quantization for memory savings\n",
    "load_in_4bit: true\n",
    "load_in_8bit: false\n",
    "\n",
    "training_type: dpo\n",
    "max_steps: 50       # Shorter for faster CI\n",
    "batch_size: 1       # Smaller batch size\n",
    "gradient_accumulation_steps: 8  # Compensate with gradient accumulation\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 128     # Shorter sequences\n",
    "warmup_steps: 5\n",
    "\n",
    "beta: 0.1\n",
    "max_prompt_length: 32\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 256\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./canary_output\n",
    "metrics_warmup_steps: 5\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/dpo_ci_4bit.yaml', 'w') as f:\n",
    "    f.write(ci_config)\n",
    "\n",
    "print(\"Created memory-optimized CI config: configs/dpo_ci_4bit.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the memory-optimized config\n",
    "!python -m canary.cli run configs/dpo_ci_4bit.yaml -o ./canary_output/ci_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage of optimized config\n",
    "ci_path = next(Path('./canary_output/ci_4bit').rglob('metrics.json'))\n",
    "\n",
    "with open(ci_path) as f:\n",
    "    ci_metrics = json.load(f)\n",
    "\n",
    "print(\"Memory-Optimized CI Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Peak memory: {ci_metrics['perf']['max_mem_mb']:.0f} MB\")\n",
    "print(f\"Step time: {ci_metrics['perf']['step_time']['mean']:.4f}s\")\n",
    "print(f\"Total duration: {ci_metrics['duration_seconds']:.1f}s\")\n",
    "\n",
    "print(f\"\\nMemory savings vs FP16: {(1 - ci_metrics['perf']['max_mem_mb']/fp16_mem)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI Workflow Example\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/canary.yml\n",
    "name: Canary Tests\n",
    "\n",
    "on: [pull_request]\n",
    "\n",
    "jobs:\n",
    "  canary:\n",
    "    runs-on: [self-hosted, gpu-limited]  # 8GB GPU runner\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Run 4-bit canary\n",
    "        run: canary run configs/dpo_ci_4bit.yaml -o ./output\n",
    "        \n",
    "      - name: Compare to 4-bit baseline\n",
    "        run: |\n",
    "          METRICS=$(find ./output -name metrics.json)\n",
    "          canary compare $METRICS baselines/limited_gpu/4bit/main.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **4-bit quantization** saves ~75% memory with slight speed impact\n",
    "2. **8-bit quantization** saves ~50% memory with minimal speed impact\n",
    "3. **Always compare like-to-like** - don't compare 4-bit runs to FP16 baselines\n",
    "4. **Organize baselines by GPU and quantization** for proper comparisons\n",
    "5. **Use `save-baseline`** command for proper baseline management\n",
    "6. **Optimize configs for CI** - smaller LoRA, shorter sequences, smaller batches\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```yaml\n",
    "# Enable 4-bit quantization\n",
    "load_in_4bit: true\n",
    "load_in_8bit: false\n",
    "\n",
    "# Enable 8-bit quantization\n",
    "load_in_4bit: false\n",
    "load_in_8bit: true\n",
    "\n",
    "# Full precision (default)\n",
    "load_in_4bit: false\n",
    "load_in_8bit: false\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Save baseline\n",
    "canary save-baseline ./output/metrics.json baselines/t4/4bit/main.json\n",
    "\n",
    "# Compare (use matching baseline!)\n",
    "canary compare ./current/metrics.json baselines/t4/4bit/main.json\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [01_quickstart.ipynb](01_quickstart.ipynb) - Core workflow basics\n",
    "- [07_ci_cd_integration.ipynb](07_ci_cd_integration.ipynb) - GitHub integration\n",
    "- [08_configuration_and_thresholds.ipynb](08_configuration_and_thresholds.ipynb) - Advanced configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}