{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mmcmanus1/rlhf-canary/blob/main/notebooks/02_profiler_deep_dive.ipynb)\n\n# Profiler Deep Dive: Diagnosing Training Slowdowns\n\nUse PyTorch's built-in profiler to pinpoint exactly where your training time is going. This notebook shows you how to identify GPU bottlenecks, CPU overhead, and memory transfer issues.\n\n**What you'll learn:**\n1. How to enable profiling in your canary configs\n2. Running a profiled training job\n3. Interpreting profiler output (CUDA vs CPU time)\n4. Identifying top operations consuming time\n5. Exporting traces for TensorBoard visualization\n6. Practical workflow: \"My run got slower - here's how to find why\"\n\n**Requirements:** GPU runtime (Runtime > Change runtime type > T4 GPU)\n\n**Runtime:** ~10-15 minutes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport sys\n\nprint(\"Starting Environment Setup...\")\n\n# --- 1. Clone or update the repo ---\nif not os.path.exists(\"/content/rlhf-canary\"):\n    !git clone https://github.com/mmcmanus1/rlhf-canary.git /content/rlhf-canary\nelse:\n    !cd /content/rlhf-canary && git pull --ff-only\n\n%cd /content/rlhf-canary\n\n# --- 2. Force-Install the \"Safe Harbor\" Stack ---\n!pip install \"trl==0.11.4\" \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"tokenizers==0.19.1\" --force-reinstall --no-deps --quiet\n!pip install -q datasets pydantic click PyYAML bitsandbytes\nprint(\"Libraries installed (TRL 0.11.4 / Transformers 4.44.2)\")\n\n# --- 3. Patch pyproject.toml (Prevent future drift) ---\nproject_file = \"/content/rlhf-canary/pyproject.toml\"\nif os.path.exists(project_file):\n    with open(project_file, \"r\") as f:\n        content = f.read()\n    \n    if \"trl==0.11.4\" not in content:\n        content = re.sub(r'trl[<>=!~]+[\\d\\.]+', 'trl==0.11.4', content)\n        with open(project_file, \"w\") as f:\n            f.write(content)\n        print(\"Config file patched to lock TRL 0.11.4\")\n\n# --- 4. Patch Source Code (Compatibility Fix) ---\nrunner_file = \"/content/rlhf-canary/canary/runner/local.py\"\nif os.path.exists(runner_file):\n    with open(runner_file, \"r\") as f:\n        code = f.read()\n    \n    if \"processing_class=\" in code:\n        code = code.replace(\"processing_class=\", \"tokenizer=\")\n        with open(runner_file, \"w\") as f:\n            f.write(code)\n        print(\"Code patched: Reverted 'processing_class' to 'tokenizer'\")\n    else:\n        print(\"Code is already compatible.\")\n\n# --- 5. Install the package ---\n!pip install -e . --quiet\n\nprint(\"Environment Ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU and installation\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "import canary\n",
    "print(f\"Canary module loaded from: {canary.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Profiler\n",
    "\n",
    "The RLHF Canary profiler captures detailed timing information during training:\n",
    "\n",
    "- **CUDA operations**: GPU kernel execution times (matrix multiplications, attention, etc.)\n",
    "- **CPU operations**: Python/C++ operations (data loading, tokenization, etc.)\n",
    "- **Memory profiling**: Memory allocation patterns\n",
    "\n",
    "### Profiler Configuration\n",
    "\n",
    "```yaml\n",
    "profiler:\n",
    "  enabled: true            # Turn on profiling\n",
    "  start_step: 50           # Start capturing at step 50 (skip warmup)\n",
    "  num_steps: 20            # Capture 20 steps\n",
    "  output_dir: ./profiler   # Where to save traces\n",
    "  record_shapes: true      # Record tensor shapes\n",
    "  profile_memory: true     # Track memory allocations\n",
    "  with_stack: false        # Stack traces (slow, disable for speed)\n",
    "```\n",
    "\n",
    "**Key insight**: We skip the first ~50 steps because they include one-time costs (JIT compilation, memory allocation) that aren't representative of steady-state performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Profiling Configuration\n",
    "\n",
    "Let's create a config that enables the profiler. We'll use a shorter run (80 steps) and profile steps 50-70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_config = \"\"\"\n",
    "name: dpo_profiled\n",
    "description: DPO training with profiler enabled for performance analysis\n",
    "\n",
    "# Model configuration\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# Training configuration\n",
    "training_type: dpo\n",
    "max_steps: 80\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 10\n",
    "\n",
    "# DPO-specific\n",
    "beta: 0.1\n",
    "max_prompt_length: 64\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 512\n",
    "seed: 42\n",
    "\n",
    "# Output configuration\n",
    "output_dir: ./profiler_output\n",
    "metrics_warmup_steps: 10\n",
    "\n",
    "# PROFILER ENABLED!\n",
    "profiler:\n",
    "  enabled: true\n",
    "  start_step: 50      # Start after warmup\n",
    "  num_steps: 20       # Profile 20 steps\n",
    "  output_dir: ./profiler_traces\n",
    "  record_shapes: true\n",
    "  profile_memory: true\n",
    "  with_stack: false   # Keep false for speed\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/dpo_profiled.yaml', 'w') as f:\n",
    "    f.write(profiler_config)\n",
    "\n",
    "print(\"Created configs/dpo_profiled.yaml with profiler enabled\")\n",
    "print(\"\\nProfiler will capture steps 50-70 (20 steps total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Training with Profiling\n",
    "\n",
    "This will take ~8-10 minutes. The profiler adds minimal overhead (~5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m canary.cli run configs/dpo_profiled.yaml -o ./profiler_output/run1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Profiler Output\n",
    "\n",
    "The profiler produces:\n",
    "1. **ProfilerSummary** in metrics.json - High-level stats\n",
    "2. **trace.json** - Detailed trace for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\n# Find the metrics file\nmetrics_paths = list(Path('./profiler_output/run1').rglob('metrics.json'))\nif not metrics_paths:\n    raise FileNotFoundError(\"No metrics.json found. Did the training run complete?\")\n\nmetrics_path = metrics_paths[0]\nprint(f\"Metrics file: {metrics_path}\")\n\nwith open(metrics_path) as f:\n    metrics = json.load(f)\n\n# Check if profiler summary exists\nif 'profiler' in metrics and metrics['profiler']:\n    profiler_summary = metrics['profiler']\n    print(\"\\n\" + \"=\"*60)\n    print(\"PROFILER SUMMARY\")\n    print(\"=\"*60)\n    print(f\"\\nTrace file: {profiler_summary.get('trace_path', 'N/A')}\")\n    print(f\"\\nTotal CUDA time: {profiler_summary.get('cuda_time_total_ms', 0):.2f} ms\")\n    print(f\"Total CPU time:  {profiler_summary.get('cpu_time_total_ms', 0):.2f} ms\")\n    print(f\"Self CUDA time:  {profiler_summary.get('self_cuda_time_total_ms', 0):.2f} ms\")\nelse:\n    print(\"No profiler summary found in metrics\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top CUDA operations (GPU bottlenecks)\n",
    "if 'profiler' in metrics and metrics['profiler']:\n",
    "    profiler_summary = metrics['profiler']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 10 CUDA OPERATIONS (GPU time)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Operation':<40} {'Time (ms)':>10} {'Count':>8}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for op in profiler_summary.get('top_cuda_ops', [])[:10]:\n",
    "        name = op['name'][:38] if len(op['name']) > 38 else op['name']\n",
    "        print(f\"{name:<40} {op['self_cuda_time_ms']:>10.2f} {op['count']:>8}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 10 CPU OPERATIONS (CPU time)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Operation':<40} {'Time (ms)':>10} {'Count':>8}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for op in profiler_summary.get('top_cpu_ops', [])[:10]:\n",
    "        name = op['name'][:38] if len(op['name']) > 38 else op['name']\n",
    "        print(f\"{name:<40} {op['self_cpu_time_ms']:>10.2f} {op['count']:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpreting the Results\n",
    "\n",
    "### What to look for:\n",
    "\n",
    "**CUDA operations (GPU)**:\n",
    "- `aten::mm`, `aten::bmm` - Matrix multiplications (expected to be high)\n",
    "- `aten::_scaled_dot_product_flash_attention` - Attention (expected to be high)\n",
    "- `aten::copy_` - Memory transfers (should NOT dominate - indicates CPU/GPU sync issues)\n",
    "\n",
    "**CPU operations**:\n",
    "- `aten::to`, `aten::_to_copy` - Tensor type conversions (high = dtype issues)\n",
    "- `Optimizer.step` - Should be reasonable, not dominant\n",
    "- High CPU time with low GPU util = dataloader/tokenization bottleneck\n",
    "\n",
    "### Warning signs:\n",
    "1. **Memory copies dominating**: CPU<->GPU transfers are slow\n",
    "2. **`to` operations high**: Dtype mismatches causing conversions\n",
    "3. **CPU >> CUDA time**: GPU is idle waiting for data\n",
    "4. **Single operation dominates**: Optimization opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick diagnosis based on profiler results\nif 'profiler' in metrics and metrics['profiler']:\n    profiler_summary = metrics['profiler']\n    \n    cuda_time = profiler_summary.get('cuda_time_total_ms', 0)\n    cpu_time = profiler_summary.get('cpu_time_total_ms', 0)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"QUICK DIAGNOSIS\")\n    print(\"=\"*60)\n    \n    # Check CPU vs GPU balance\n    if cpu_time > 0 and cuda_time > 0:\n        ratio = cpu_time / cuda_time\n        if ratio > 2.0:\n            print(f\"\\nWARNING: CPU time ({cpu_time:.0f}ms) >> CUDA time ({cuda_time:.0f}ms)\")\n            print(\"This suggests a CPU bottleneck (dataloader, tokenization, or Python overhead)\")\n        elif ratio < 0.5:\n            print(f\"\\nGOOD: CUDA time ({cuda_time:.0f}ms) >> CPU time ({cpu_time:.0f}ms)\")\n            print(\"GPU is well-utilized, training is compute-bound (optimal)\")\n        else:\n            print(f\"\\nBALANCED: CPU time ({cpu_time:.0f}ms) ~ CUDA time ({cuda_time:.0f}ms)\")\n            print(\"Mixed workload - both CPU and GPU contribute significantly\")\n    elif cuda_time == 0 and cpu_time > 0:\n        print(f\"\\nINFO: No CUDA kernel times recorded (CPU time: {cpu_time:.0f}ms)\")\n        print(\"This can happen when CUPTI profiling is unavailable (common on some cloud GPUs)\")\n        print(\"CPU-side CUDA calls were still captured - see CPU operations above\")\n    \n    # Check for memory copy dominance\n    top_cuda = profiler_summary.get('top_cuda_ops', [])\n    if top_cuda:\n        copy_ops = [op for op in top_cuda if 'copy' in op['name'].lower()]\n        total_copy_time = sum(op['self_cuda_time_ms'] for op in copy_ops)\n        total_cuda_time = sum(op['self_cuda_time_ms'] for op in top_cuda)\n        \n        if total_cuda_time == 0:\n            print(\"\\nNo CUDA kernel times recorded\")\n            print(\"This can happen when CUPTI profiling is unavailable (common on some cloud GPUs)\")\n            print(\"CPU-side CUDA calls were still captured - see CPU operations above\")\n        elif total_copy_time / total_cuda_time > 0.3:\n            print(f\"\\nWARNING: Memory copies are {total_copy_time/total_cuda_time:.0%} of CUDA time\")\n            print(\"Consider: reducing CPU<->GPU transfers, using pinned memory\")\n        else:\n            print(f\"\\nMemory copy overhead: {total_copy_time/total_cuda_time:.0%} of CUDA time (acceptable)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Trace for TensorBoard\n",
    "\n",
    "For deeper analysis, you can visualize the trace in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the trace file\n",
    "trace_files = list(Path('./profiler_traces').rglob('trace.json'))\n",
    "\n",
    "if trace_files:\n",
    "    trace_path = trace_files[0]\n",
    "    print(f\"Trace file found: {trace_path}\")\n",
    "    print(f\"File size: {trace_path.stat().st_size / 1024:.1f} KB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TO VIEW IN TENSORBOARD:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n1. Download the trace file to your local machine\")\n",
    "    print(\"2. Run: tensorboard --logdir=./profiler_traces\")\n",
    "    print(\"3. Go to the 'PyTorch Profiler' tab\")\n",
    "    print(\"\\nOr view in Chrome:\")\n",
    "    print(\"1. Open chrome://tracing\")\n",
    "    print(\"2. Load the trace.json file\")\n",
    "else:\n",
    "    print(\"No trace file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Workflow: Diagnosing a Slowdown\n",
    "\n",
    "Let's simulate a real debugging scenario. First, let's get baseline metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current run as baseline for comparison\n",
    "baseline_metrics = metrics.copy()\n",
    "\n",
    "print(\"Baseline metrics:\")\n",
    "print(f\"  Step time (mean): {baseline_metrics['perf']['step_time']['mean']:.4f}s\")\n",
    "print(f\"  Tokens/sec: {baseline_metrics['perf']['approx_tokens_per_sec']:.0f}\")\n",
    "print(f\"  Peak memory: {baseline_metrics['perf']['max_mem_mb']:.0f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"slow\" config (simulating a regression)\n",
    "slow_config = \"\"\"\n",
    "name: dpo_slow_profiled\n",
    "description: Intentionally slow config to demonstrate regression detection\n",
    "\n",
    "# Model configuration\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# Training configuration - SMALLER BATCH = SLOWER\n",
    "training_type: dpo\n",
    "max_steps: 80\n",
    "batch_size: 1              # Reduced from 2!\n",
    "gradient_accumulation_steps: 8  # Increased to compensate\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 10\n",
    "\n",
    "# DPO-specific\n",
    "beta: 0.1\n",
    "max_prompt_length: 64\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 512\n",
    "seed: 42\n",
    "\n",
    "# Output configuration\n",
    "output_dir: ./profiler_output\n",
    "metrics_warmup_steps: 10\n",
    "\n",
    "# PROFILER to diagnose the slowdown\n",
    "profiler:\n",
    "  enabled: true\n",
    "  start_step: 50\n",
    "  num_steps: 20\n",
    "  output_dir: ./profiler_traces_slow\n",
    "  record_shapes: true\n",
    "  profile_memory: true\n",
    "  with_stack: false\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/dpo_slow_profiled.yaml', 'w') as f:\n",
    "    f.write(slow_config)\n",
    "\n",
    "print(\"Created slow config with batch_size=1 (was 2)\")\n",
    "print(\"This will cause a throughput regression...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the slow config\n",
    "!python -m canary.cli run configs/dpo_slow_profiled.yaml -o ./profiler_output/slow_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare the profiler outputs\nslow_metrics_paths = list(Path('./profiler_output/slow_run').rglob('metrics.json'))\nif not slow_metrics_paths:\n    raise FileNotFoundError(\"No metrics.json found for slow run. Did the training complete?\")\n\nslow_metrics_path = slow_metrics_paths[0]\nwith open(slow_metrics_path) as f:\n    slow_metrics = json.load(f)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARISON: Baseline vs Slow Run\")\nprint(\"=\"*60)\n\nprint(f\"\\n{'Metric':<25} {'Baseline':>15} {'Slow':>15} {'Change':>15}\")\nprint(\"-\"*70)\n\n# Step time\nbase_step = baseline_metrics['perf']['step_time']['mean']\nslow_step = slow_metrics['perf']['step_time']['mean']\nchange = (slow_step - base_step) / base_step * 100\nprint(f\"{'Step time (s)':<25} {base_step:>15.4f} {slow_step:>15.4f} {change:>+14.1f}%\")\n\n# Tokens/sec\nbase_tps = baseline_metrics['perf']['approx_tokens_per_sec']\nslow_tps = slow_metrics['perf']['approx_tokens_per_sec']\nchange = (slow_tps - base_tps) / base_tps * 100\nprint(f\"{'Tokens/sec':<25} {base_tps:>15.0f} {slow_tps:>15.0f} {change:>+14.1f}%\")\n\n# Memory\nbase_mem = baseline_metrics['perf']['max_mem_mb']\nslow_mem = slow_metrics['perf']['max_mem_mb']\nchange = slow_mem - base_mem\nprint(f\"{'Memory (MB)':<25} {base_mem:>15.0f} {slow_mem:>15.0f} {change:>+14.0f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare profiler details\n",
    "if 'profiler' in slow_metrics and slow_metrics['profiler']:\n",
    "    slow_profiler = slow_metrics['profiler']\n",
    "    base_profiler = baseline_metrics.get('profiler', {})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROFILER COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<25} {'Baseline':>15} {'Slow':>15}\")\n",
    "    print(\"-\"*55)\n",
    "    \n",
    "    base_cuda = base_profiler.get('cuda_time_total_ms', 0)\n",
    "    slow_cuda = slow_profiler.get('cuda_time_total_ms', 0)\n",
    "    print(f\"{'CUDA time (ms)':<25} {base_cuda:>15.2f} {slow_cuda:>15.2f}\")\n",
    "    \n",
    "    base_cpu = base_profiler.get('cpu_time_total_ms', 0)\n",
    "    slow_cpu = slow_profiler.get('cpu_time_total_ms', 0)\n",
    "    print(f\"{'CPU time (ms)':<25} {base_cpu:>15.2f} {slow_cpu:>15.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DIAGNOSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nThe slowdown is caused by smaller batch size:\")\n",
    "    print(\"- More Python/framework overhead per sample\")\n",
    "    print(\"- Less efficient GPU utilization (smaller kernels)\")\n",
    "    print(\"- More dataloader calls per effective batch\")\n",
    "    print(\"\\nFix: Increase batch_size, use gradient accumulation wisely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Enable profiling** with `profiler.enabled: true` in your config\n",
    "2. **Skip warmup steps** - profile steady-state (start_step: 50+)\n",
    "3. **Keep it short** - 20 steps is usually enough\n",
    "4. **Check the ratio** - CPU time >> CUDA time = bottleneck\n",
    "5. **Watch for memory copies** - they indicate sync issues\n",
    "6. **Use TensorBoard** for detailed timeline analysis\n",
    "\n",
    "### When to Profile:\n",
    "\n",
    "- After a performance regression is detected\n",
    "- When optimizing training code\n",
    "- When debugging \"my run got slower after N steps\"\n",
    "- Before and after major code changes\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- See `03_stability_monitoring.ipynb` for detecting training instabilities\n",
    "- See `04_root_cause_analysis.ipynb` for the full regression debugging workflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}