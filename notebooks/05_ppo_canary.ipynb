{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Canary: RL-Specific Training Validation\n",
    "\n",
    "This notebook demonstrates how to use RLHF Canary for PPO (Proximal Policy Optimization) training validation.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How PPO training differs from DPO/SFT\n",
    "2. Running PPO canaries with synthetic rewards\n",
    "3. Understanding PPO-specific metrics (KL, entropy, clip fraction)\n",
    "4. Memory considerations: PPO uses ~3x memory vs SFT\n",
    "5. PPO failure modes and how to detect them\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime > Change runtime type > T4 GPU)\n",
    "\n",
    "**Runtime:** ~15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "print(\"Starting Environment Setup...\")\n",
    "\n",
    "# --- 1. Clone the repo first ---\n",
    "if not os.path.exists(\"/content/rlhf-canary\"):\n",
    "    !git clone https://github.com/mmcmanus1/rlhf-canary.git /content/rlhf-canary\n",
    "\n",
    "%cd /content/rlhf-canary\n",
    "\n",
    "# --- 2. Force-Install the \"Safe Harbor\" Stack ---\n",
    "!pip install \"trl==0.11.4\" \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"tokenizers==0.19.1\" --force-reinstall --no-deps --quiet\n",
    "!pip install -q datasets pydantic click PyYAML bitsandbytes\n",
    "print(\"Libraries installed (TRL 0.11.4 / Transformers 4.44.2)\")\n",
    "\n",
    "# --- 3. Patch pyproject.toml (Prevent future drift) ---\n",
    "project_file = \"/content/rlhf-canary/pyproject.toml\"\n",
    "if os.path.exists(project_file):\n",
    "    with open(project_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    if \"trl==0.11.4\" not in content:\n",
    "        content = re.sub(r'trl[<>=!~]+[\\d\\.]+', 'trl==0.11.4', content)\n",
    "        with open(project_file, \"w\") as f:\n",
    "            f.write(content)\n",
    "        print(\"Config file patched to lock TRL 0.11.4\")\n",
    "\n",
    "# --- 4. Patch Source Code (Compatibility Fix) ---\n",
    "runner_file = \"/content/rlhf-canary/canary/runner/local.py\"\n",
    "if os.path.exists(runner_file):\n",
    "    with open(runner_file, \"r\") as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    if \"processing_class=\" in code:\n",
    "        code = code.replace(\"processing_class=\", \"tokenizer=\")\n",
    "        with open(runner_file, \"w\") as f:\n",
    "            f.write(code)\n",
    "        print(\"Code patched: Reverted 'processing_class' to 'tokenizer'\")\n",
    "    else:\n",
    "        print(\"Code is already compatible.\")\n",
    "\n",
    "# --- 5. Install the package ---\n",
    "!pip install -e . --quiet\n",
    "\n",
    "print(\"Environment Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU and installation\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Memory: {mem_gb:.1f} GB\")\n",
    "    \n",
    "    # Warning for low memory\n",
    "    if mem_gb < 12:\n",
    "        print(\"\\nNOTE: PPO uses ~3x memory vs SFT (model + ref_model + value_head)\")\n",
    "        print(\"On T4 (16GB), PPO with pythia-70m should work fine.\")\n",
    "\n",
    "import canary\n",
    "print(f\"Canary module loaded from: {canary.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding PPO Training\n",
    "\n",
    "PPO (Proximal Policy Optimization) is fundamentally different from DPO/SFT:\n",
    "\n",
    "### PPO vs DPO/SFT\n",
    "\n",
    "| Aspect | SFT/DPO | PPO |\n",
    "|--------|---------|-----|\n",
    "| Training loop | Forward + backward | Generate → Reward → Update |\n",
    "| Memory usage | ~1-2x model | ~3x model |\n",
    "| Components | Model (+ ref for DPO) | Model + ref + value head |\n",
    "| Reward signal | Implicit (preferences) | Explicit (reward model) |\n",
    "| Key metrics | Loss, KL | Policy loss, value loss, KL, entropy, clip fraction |\n",
    "\n",
    "### PPO-Specific Metrics\n",
    "\n",
    "| Metric | What it measures | Healthy range |\n",
    "|--------|-----------------|---------------|\n",
    "| `objective/kl` | Policy drift from reference | < target_kl (default: 6.0) |\n",
    "| `objective/entropy` | Policy randomness | Decreasing slowly, not collapsing |\n",
    "| `ppo/clipfrac` | Update clipping frequency | < 0.2 is normal |\n",
    "| `ppo/policy_loss` | Actor loss | Stable, no NaN |\n",
    "| `ppo/value_loss` | Critic loss | Stable, no NaN |\n",
    "\n",
    "### Memory Architecture\n",
    "\n",
    "```\n",
    "PPO Memory = Policy Model + Reference Model + Value Head\n",
    "           ≈ 1x model    + 1x model        + small overhead\n",
    "           ≈ 3x SFT memory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO Configuration Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the PPO smoke config\n",
    "!cat configs/ppo_smoke.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain key PPO parameters\n",
    "print(\"PPO Configuration Parameters:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "params = [\n",
    "    (\"ppo_epochs\", \"4\", \"PPO update epochs per batch (more = more aggressive)\"),\n",
    "    (\"init_kl_coef\", \"0.2\", \"Initial KL penalty coefficient\"),\n",
    "    (\"target_kl\", \"6.0\", \"Target KL for adaptive penalty\"),\n",
    "    (\"cliprange\", \"0.2\", \"Policy ratio clipping range\"),\n",
    "    (\"vf_coef\", \"0.1\", \"Value function loss weight\"),\n",
    "    (\"max_new_tokens\", \"64\", \"Tokens to generate per prompt\"),\n",
    "    (\"use_synthetic_reward\", \"true\", \"Use length-based rewards (for canary)\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Parameter':<20} {'Default':>10} {'Description'}\")\n",
    "print(\"-\"*70)\n",
    "for name, default, desc in params:\n",
    "    print(f\"{name:<20} {default:>10} {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run PPO Baseline Canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PPO smoke test (~10-15 min)\n",
    "!python -m canary.cli run configs/ppo_smoke.yaml -o ./ppo_output/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\n# Load and display PPO metrics\nbaseline_paths = list(Path('./ppo_output/baseline').rglob('metrics.json'))\nif not baseline_paths:\n    raise FileNotFoundError(\"No metrics.json found for PPO baseline. Did the training complete?\")\n\nbaseline_path = baseline_paths[0]\nwith open(baseline_path) as f:\n    ppo_metrics = json.load(f)\n\nprint(\"=\"*60)\nprint(\"PPO BASELINE METRICS\")\nprint(\"=\"*60)\n\nprint(f\"\\nPerformance:\")\nprint(f\"  Step time (mean): {ppo_metrics['perf']['step_time']['mean']:.4f}s\")\nprint(f\"  Tokens/sec: {ppo_metrics['perf']['approx_tokens_per_sec']:.0f}\")\nprint(f\"  Peak memory: {ppo_metrics['perf']['max_mem_mb']:.0f}MB\")\n\nprint(f\"\\nStability:\")\nprint(f\"  NaN steps: {ppo_metrics['stability']['nan_steps']}\")\nprint(f\"  Inf steps: {ppo_metrics['stability']['inf_steps']}\")\nprint(f\"  Loss diverged: {ppo_metrics['stability']['loss_diverged']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PPO vs DPO: Memory Comparison\n",
    "\n",
    "Let's compare memory usage between PPO and DPO to see the ~3x difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a DPO smoke test for comparison\n",
    "!python -m canary.cli run configs/dpo_smoke.yaml -o ./ppo_output/dpo_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load DPO metrics\ndpo_paths = list(Path('./ppo_output/dpo_comparison').rglob('metrics.json'))\nif not dpo_paths:\n    raise FileNotFoundError(\"No metrics.json found for DPO comparison. Did the training complete?\")\n\ndpo_path = dpo_paths[0]\nwith open(dpo_path) as f:\n    dpo_metrics = json.load(f)\n\nprint(\"=\"*60)\nprint(\"MEMORY COMPARISON: PPO vs DPO\")\nprint(\"=\"*60)\n\nppo_mem = ppo_metrics['perf']['max_mem_mb']\ndpo_mem = dpo_metrics['perf']['max_mem_mb']\n\n# Sanity check for valid memory values\nif dpo_mem > 100:  # Reasonable minimum memory in MB\n    ratio = ppo_mem / dpo_mem\nelse:\n    print(\"Warning: DPO memory metrics appear invalid\")\n    ratio = 0\n\nprint(f\"\\n{'Training Type':<15} {'Peak Memory':>15} {'Relative':>15}\")\nprint(\"-\"*45)\nprint(f\"{'DPO':<15} {dpo_mem:>14.0f}MB {'1.0x':>15}\")\nprint(f\"{'PPO':<15} {ppo_mem:>14.0f}MB {f'{ratio:.1f}x':>15}\")\n\nprint(f\"\\nPPO uses {ratio:.1f}x more memory than DPO\")\nprint(\"This is expected: PPO = policy + reference + value head\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing PPO Runs\n",
    "\n",
    "Let's save the baseline and run another PPO canary for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline\n",
    "!mkdir -p baselines\n",
    "!cp {baseline_path} baselines/ppo_baseline.json\n",
    "print(f\"Saved PPO baseline to baselines/ppo_baseline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a slightly different config for comparison\n",
    "ppo_run2_config = \"\"\"\n",
    "name: ppo_run2\n",
    "description: Second PPO run for comparison\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "training_type: ppo\n",
    "max_steps: 50\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 1.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 5\n",
    "\n",
    "# Same PPO params as baseline\n",
    "ppo_epochs: 4\n",
    "init_kl_coef: 0.2\n",
    "target_kl: 6.0\n",
    "cliprange: 0.2\n",
    "vf_coef: 0.1\n",
    "max_prompt_length: 64\n",
    "max_new_tokens: 64\n",
    "use_synthetic_reward: true\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 256\n",
    "seed: 123  # Different seed\n",
    "\n",
    "output_dir: ./ppo_output\n",
    "metrics_warmup_steps: 5\n",
    "\n",
    "profiler:\n",
    "  enabled: false\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/ppo_run2.yaml', 'w') as f:\n",
    "    f.write(ppo_run2_config)\n",
    "\n",
    "print(\"Created second PPO config with different seed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run second PPO canary\n",
    "!python -m canary.cli run configs/ppo_run2.yaml -o ./ppo_output/run2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare to baseline\nrun2_paths = list(Path('./ppo_output/run2').rglob('metrics.json'))\nif not run2_paths:\n    raise FileNotFoundError(\"No metrics.json found for run2. Did the training complete?\")\n\nrun2_path = run2_paths[0]\n!python -m canary.cli compare {run2_path} baselines/ppo_baseline.json --threshold-tier smoke"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PPO Failure Modes\n",
    "\n",
    "PPO has specific failure modes to watch for:\n",
    "\n",
    "### 1. KL Explosion\n",
    "- **Symptom**: `objective/kl` >> target_kl\n",
    "- **Cause**: Policy drifting too far from reference\n",
    "- **Fix**: Increase `init_kl_coef`, reduce learning rate\n",
    "\n",
    "### 2. Entropy Collapse\n",
    "- **Symptom**: `objective/entropy` drops to near zero\n",
    "- **Cause**: Policy becoming deterministic too fast\n",
    "- **Fix**: Add entropy bonus, check reward signal\n",
    "\n",
    "### 3. Value Function Divergence\n",
    "- **Symptom**: `ppo/value_loss` exploding\n",
    "- **Cause**: Value head not learning properly\n",
    "- **Fix**: Increase `vf_coef`, check reward scale\n",
    "\n",
    "### 4. High Clip Fraction\n",
    "- **Symptom**: `ppo/clipfrac` > 0.3 consistently\n",
    "- **Cause**: Policy updates too aggressive\n",
    "- **Fix**: Reduce learning rate or `cliprange`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an unstable PPO config to demonstrate failure modes\n",
    "unstable_ppo_config = \"\"\"\n",
    "name: ppo_unstable\n",
    "description: Intentionally unstable PPO settings\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "training_type: ppo\n",
    "max_steps: 30\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-4   # 5x higher LR!\n",
    "max_length: 256\n",
    "warmup_steps: 2\n",
    "\n",
    "# UNSTABLE PPO settings\n",
    "ppo_epochs: 8           # More aggressive updates\n",
    "init_kl_coef: 0.05      # Weak KL penalty\n",
    "target_kl: 20.0         # Very high target\n",
    "cliprange: 0.3          # Wider clipping\n",
    "vf_coef: 0.1\n",
    "max_prompt_length: 64\n",
    "max_new_tokens: 64\n",
    "use_synthetic_reward: true\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 128\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./ppo_output\n",
    "metrics_warmup_steps: 2\n",
    "\n",
    "profiler:\n",
    "  enabled: false\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/ppo_unstable.yaml', 'w') as f:\n",
    "    f.write(unstable_ppo_config)\n",
    "\n",
    "print(\"Created unstable PPO config:\")\n",
    "print(\"  - learning_rate: 5.0e-4 (5x normal)\")\n",
    "print(\"  - ppo_epochs: 8 (2x normal)\")\n",
    "print(\"  - init_kl_coef: 0.05 (weak KL penalty)\")\n",
    "print(\"  - target_kl: 20.0 (very high)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run unstable config\n",
    "!python -m canary.cli run configs/ppo_unstable.yaml -o ./ppo_output/unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare unstable run to baseline\n",
    "unstable_paths = list(Path('./ppo_output/unstable').rglob('metrics.json'))\n",
    "\n",
    "if unstable_paths:\n",
    "    unstable_path = unstable_paths[0]\n",
    "    with open(unstable_path) as f:\n",
    "        unstable_metrics = json.load(f)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"UNSTABLE PPO RUN ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nStability Metrics:\")\n",
    "    print(f\"  NaN steps: {unstable_metrics['stability']['nan_steps']}\")\n",
    "    print(f\"  Inf steps: {unstable_metrics['stability']['inf_steps']}\")\n",
    "    print(f\"  Loss diverged: {unstable_metrics['stability']['loss_diverged']}\")\n",
    "    \n",
    "    print(f\"\\nPerformance Comparison:\")\n",
    "    print(f\"  Baseline step time: {ppo_metrics['perf']['step_time']['mean']:.4f}s\")\n",
    "    print(f\"  Unstable step time: {unstable_metrics['perf']['step_time']['mean']:.4f}s\")\n",
    "    \n",
    "    # Run formal comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FORMAL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    !python -m canary.cli compare {unstable_path} baselines/ppo_baseline.json --threshold-tier smoke\n",
    "else:\n",
    "    print(\"Unstable run failed to produce metrics (crashed during training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PPO Canary Best Practices\n",
    "\n",
    "### Configuration Tips\n",
    "\n",
    "```yaml\n",
    "# Conservative settings for stable canaries\n",
    "ppo_epochs: 4          # Standard value\n",
    "init_kl_coef: 0.2      # Strong KL penalty\n",
    "target_kl: 6.0         # Reasonable target\n",
    "cliprange: 0.2         # Standard clipping\n",
    "learning_rate: 1.0e-5  # Conservative LR\n",
    "```\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "1. Use PEFT/LoRA to reduce memory\n",
    "2. Use 4-bit quantization for larger models\n",
    "3. Keep batch_size small on limited VRAM\n",
    "4. Monitor peak memory in canary reports\n",
    "\n",
    "### Synthetic vs Real Rewards\n",
    "\n",
    "The canary uses **synthetic rewards** (length-based) for testing:\n",
    "- Pros: No need for reward model, fast, consistent\n",
    "- Cons: Doesn't test reward model integration\n",
    "\n",
    "For production, you'd use a trained reward model, but for regression testing, synthetic rewards are sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available PPO configs\n",
    "print(\"Available PPO Configurations:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "configs = [\n",
    "    (\"ppo_smoke.yaml\", \"~10-15 min\", \"PR gating, quick validation\"),\n",
    "    (\"ppo_perf.yaml\", \"~45-60 min\", \"Performance regression detection\"),\n",
    "    (\"ppo_nightly.yaml\", \"~90-120 min\", \"Comprehensive soak test\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Config':<20} {'Runtime':>15} {'Use Case'}\")\n",
    "print(\"-\"*60)\n",
    "for config, runtime, use_case in configs:\n",
    "    print(f\"{config:<20} {runtime:>15} {use_case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **PPO uses ~3x memory** vs SFT (model + ref + value head)\n",
    "2. **Synthetic rewards** allow canary testing without a reward model\n",
    "3. **PPO-specific metrics**: KL, entropy, clip fraction, policy/value loss\n",
    "4. **Failure modes**: KL explosion, entropy collapse, high clip fraction\n",
    "5. **Conservative settings** ensure stable canaries\n",
    "\n",
    "### When to Use PPO Canaries:\n",
    "\n",
    "- After changes to PPO training code\n",
    "- After TRL/HuggingFace library updates\n",
    "- When optimizing PPO hyperparameters\n",
    "- As part of nightly regression testing\n",
    "\n",
    "### Configuration Tiers:\n",
    "\n",
    "| Tier | Steps | Runtime | Use Case |\n",
    "|------|-------|---------|----------|\n",
    "| Smoke | 50 | ~15 min | PR gating |\n",
    "| Perf | 200 | ~60 min | Performance analysis |\n",
    "| Nightly | 500 | ~2 hr | Soak testing |\n",
    "\n",
    "### Related Notebooks:\n",
    "\n",
    "- `02_profiler_deep_dive.ipynb` - Performance profiling (DPO/SFT)\n",
    "- `03_stability_monitoring.ipynb` - Stability metrics and failure detection\n",
    "- `04_root_cause_analysis.ipynb` - Debugging regression causes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}