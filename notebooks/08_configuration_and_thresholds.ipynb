{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mmcmanus1/rlhf-canary/blob/main/notebooks/08_configuration_and_thresholds.ipynb)\n",
    "\n",
    "# Configuration & Thresholds: Customizing Your Canary Tests\n",
    "\n",
    "Master RLHF Canary configuration: generate configs, customize regression thresholds, understand test tiers, and build test matrices for comprehensive coverage.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Using `canary init-config` to generate starter configurations\n",
    "2. Understanding threshold tiers (smoke, default, perf, nightly)\n",
    "3. Creating custom threshold files\n",
    "4. Embedding thresholds in config files\n",
    "5. Understanding the test matrix system\n",
    "6. Environment fingerprinting for reproducibility\n",
    "7. JSON output for automation\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime > Change runtime type > T4 GPU)\n",
    "\n",
    "**Runtime:** ~12-15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "print(\"Starting Environment Setup...\")\n",
    "\n",
    "# --- 1. Clone the repo first ---\n",
    "if not os.path.exists(\"/content/rlhf-canary\"):\n",
    "    !git clone https://github.com/mmcmanus1/rlhf-canary.git /content/rlhf-canary\n",
    "\n",
    "%cd /content/rlhf-canary\n",
    "\n",
    "# --- 2. Force-Install the \"Safe Harbor\" Stack ---\n",
    "!pip install \"trl==0.11.4\" \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"tokenizers==0.19.1\" --force-reinstall --no-deps --quiet\n",
    "!pip install -q datasets pydantic click PyYAML bitsandbytes\n",
    "print(\"Libraries installed (TRL 0.11.4 / Transformers 4.44.2)\")\n",
    "\n",
    "# --- 3. Patch pyproject.toml ---\n",
    "project_file = \"/content/rlhf-canary/pyproject.toml\"\n",
    "if os.path.exists(project_file):\n",
    "    with open(project_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    if \"trl==0.11.4\" not in content:\n",
    "        content = re.sub(r'trl[<>=!~]+[\\d\\.]+', 'trl==0.11.4', content)\n",
    "        with open(project_file, \"w\") as f:\n",
    "            f.write(content)\n",
    "        print(\"Config file patched to lock TRL 0.11.4\")\n",
    "\n",
    "# --- 4. Patch Source Code ---\n",
    "runner_file = \"/content/rlhf-canary/canary/runner/local.py\"\n",
    "if os.path.exists(runner_file):\n",
    "    with open(runner_file, \"r\") as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    if \"processing_class=\" in code:\n",
    "        code = code.replace(\"processing_class=\", \"tokenizer=\")\n",
    "        with open(runner_file, \"w\") as f:\n",
    "            f.write(code)\n",
    "        print(\"Code patched: Reverted 'processing_class' to 'tokenizer'\")\n",
    "    else:\n",
    "        print(\"Code is already compatible.\")\n",
    "\n",
    "# --- 5. Install the package ---\n",
    "!pip install -e . --quiet\n",
    "\n",
    "print(\"Environment Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Configurations with `init-config`\n",
    "\n",
    "The `canary init-config` command generates starter configurations for different training types and test tiers.\n",
    "\n",
    "### Usage\n",
    "\n",
    "```bash\n",
    "canary init-config <output_path> --type <dpo|sft|ppo> --tier <smoke|perf|nightly>\n",
    "```\n",
    "\n",
    "### Step Counts by Tier\n",
    "\n",
    "| Tier | DPO/SFT Steps | PPO Steps | Duration | Use Case |\n",
    "|------|---------------|-----------|----------|----------|\n",
    "| smoke | 100 | 50 | ~5-10 min | PR gating |\n",
    "| perf | 500 | 200 | ~20-45 min | Performance analysis |\n",
    "| nightly | 2000 | 500 | ~1-3 hr | Comprehensive testing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DPO configs for all tiers\n",
    "!python -m canary.cli init-config generated/dpo_smoke.yaml --type dpo --tier smoke\n",
    "!python -m canary.cli init-config generated/dpo_perf.yaml --type dpo --tier perf\n",
    "!python -m canary.cli init-config generated/dpo_nightly.yaml --type dpo --tier nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PPO and SFT smoke configs\n",
    "!python -m canary.cli init-config generated/ppo_smoke.yaml --type ppo --tier smoke\n",
    "!python -m canary.cli init-config generated/sft_smoke.yaml --type sft --tier smoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a generated config\n",
    "!cat generated/dpo_smoke.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DPO vs PPO configs\n",
    "print(\"=== DPO Smoke Config ===\")\n",
    "!cat generated/dpo_smoke.yaml\n",
    "\n",
    "print(\"\\n=== PPO Smoke Config ===\")\n",
    "!cat generated/ppo_smoke.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Fields Explained\n",
    "\n",
    "| Field | Description | Default |\n",
    "|-------|-------------|---------|\n",
    "| `name` | Run identifier | `{type}_{tier}` |\n",
    "| `model_name` | HuggingFace model | `EleutherAI/pythia-70m` |\n",
    "| `use_peft` | Enable LoRA | `true` |\n",
    "| `training_type` | `dpo`, `sft`, or `ppo` | varies |\n",
    "| `max_steps` | Training steps | varies by tier |\n",
    "| `batch_size` | Per-device batch size | `2` |\n",
    "| `dataset_name` | HuggingFace dataset | `Anthropic/hh-rlhf` |\n",
    "\n",
    "**DPO-specific:** `beta`, `max_prompt_length`\n",
    "\n",
    "**PPO-specific:** `ppo_epochs`, `init_kl_coef`, `target_kl`, `cliprange`, `vf_coef`, `max_new_tokens`, `use_synthetic_reward`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Threshold Tiers\n",
    "\n",
    "Thresholds control how sensitive regression detection is. RLHF Canary provides four built-in tiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all threshold tiers programmatically\n",
    "from canary.compare.thresholds import (\n",
    "    DEFAULT_THRESHOLDS,\n",
    "    SMOKE_THRESHOLDS,\n",
    "    PERF_THRESHOLDS,\n",
    "    NIGHTLY_THRESHOLDS,\n",
    "    get_thresholds,\n",
    ")\n",
    "\n",
    "print(\"Threshold Comparison\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Metric':<35} {'Smoke':>10} {'Default':>10} {'Perf':>10} {'Nightly':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "metrics = [\n",
    "    (\"max_step_time_increase_pct\", \"%\"),\n",
    "    (\"max_tps_drop_pct\", \"%\"),\n",
    "    (\"max_mem_increase_mb\", \"MB\"),\n",
    "    (\"max_mem_increase_pct\", \"%\"),\n",
    "    (\"min_step_count\", \"steps\"),\n",
    "    (\"nan_steps_allowed\", \"steps\"),\n",
    "    (\"inf_steps_allowed\", \"steps\"),\n",
    "]\n",
    "\n",
    "tiers = [SMOKE_THRESHOLDS, DEFAULT_THRESHOLDS, PERF_THRESHOLDS, NIGHTLY_THRESHOLDS]\n",
    "\n",
    "for metric, unit in metrics:\n",
    "    values = [getattr(t, metric) for t in tiers]\n",
    "    print(f\"{metric:<35} {values[0]:>9}{unit[0]} {values[1]:>9}{unit[0]} {values[2]:>9}{unit[0]} {values[3]:>9}{unit[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Tier\n",
    "\n",
    "| Tier | Strictness | Use Case |\n",
    "|------|------------|----------|\n",
    "| **smoke** | Lenient | PR gating - catch obvious regressions quickly |\n",
    "| **default** | Balanced | General purpose, manual comparisons |\n",
    "| **perf** | Strict | Performance-focused testing, longer runs |\n",
    "| **nightly** | Strictest | Comprehensive nightly soak tests |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quick canary for comparison demos\n",
    "!python -m canary.cli run configs/dpo_smoke.yaml -o ./canary_output/baseline\n",
    "\n",
    "from pathlib import Path\n",
    "baseline_path = next(Path('./canary_output/baseline').rglob('metrics.json'))\n",
    "!mkdir -p baselines\n",
    "!cp {baseline_path} baselines/main.json\n",
    "print(f\"Baseline saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare using different tiers\n",
    "print(\"=== SMOKE TIER (lenient) ===\")\n",
    "!python -m canary.cli compare {baseline_path} baselines/main.json --threshold-tier smoke 2>/dev/null | head -20\n",
    "\n",
    "print(\"\\n=== NIGHTLY TIER (strict) ===\")\n",
    "!python -m canary.cli compare {baseline_path} baselines/main.json --threshold-tier nightly 2>/dev/null | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Custom Threshold Files\n",
    "\n",
    "For project-specific needs, create custom threshold YAML files. The format supports:\n",
    "\n",
    "1. **`base_tier`** - Start from an existing tier\n",
    "2. **Override** - Customize specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a custom threshold file\ncustom_thresholds = \"\"\"\n# Custom thresholds for noisy GPU cluster\n# Start from smoke tier (lenient) and customize\nbase_tier: smoke\n\n# Performance thresholds\n# Our cluster has high variance, so we're lenient on timing\nmax_step_time_increase_pct: 25.0  # Allow 25% increase (default smoke: 15%)\nmax_tps_drop_pct: 20.0            # Allow 20% throughput drop (default smoke: 12%)\n\n# Memory thresholds\n# We have limited VRAM, so we're strict on memory\nmax_mem_increase_mb: 256.0        # Only 256MB increase allowed (default smoke: 1000MB)\nmax_mem_increase_pct: 10.0        # Only 10% increase allowed (default smoke: 20%)\n\n# Stability thresholds\n# Zero tolerance for NaN/Inf\nnan_steps_allowed: 0\ninf_steps_allowed: 0\n\n# Minimum steps for valid comparison\nmin_step_count: 20\n\"\"\"\n\n!mkdir -p thresholds\nwith open('thresholds/noisy_cluster.yaml', 'w') as f:\n    f.write(custom_thresholds)\n\nprint(\"Created thresholds/noisy_cluster.yaml\")\n!cat thresholds/noisy_cluster.yaml"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom thresholds\n",
    "!python -m canary.cli compare {baseline_path} baselines/main.json --threshold-file thresholds/noisy_cluster.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect custom thresholds programmatically\n",
    "from canary.compare.thresholds import load_thresholds_from_yaml\n",
    "\n",
    "custom = load_thresholds_from_yaml('thresholds/noisy_cluster.yaml')\n",
    "\n",
    "print(\"Loaded custom thresholds:\")\n",
    "print(f\"  Step time increase: {custom.max_step_time_increase_pct}%\")\n",
    "print(f\"  TPS drop: {custom.max_tps_drop_pct}%\")\n",
    "print(f\"  Memory increase: {custom.max_mem_increase_mb}MB\")\n",
    "print(f\"  Min steps: {custom.min_step_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Custom Threshold Scenarios\n",
    "\n",
    "**Lenient (demo/development):**\n",
    "```yaml\n",
    "base_tier: smoke\n",
    "max_step_time_increase_pct: 50.0\n",
    "max_tps_drop_pct: 40.0\n",
    "max_mem_increase_mb: 2000.0\n",
    "```\n",
    "\n",
    "**Strict (production):**\n",
    "```yaml\n",
    "base_tier: nightly\n",
    "max_step_time_increase_pct: 3.0\n",
    "max_tps_drop_pct: 2.0\n",
    "nan_steps_allowed: 0\n",
    "```\n",
    "\n",
    "**Memory-focused:**\n",
    "```yaml\n",
    "base_tier: default\n",
    "max_mem_increase_mb: 100.0\n",
    "max_mem_increase_pct: 5.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Thresholds in Config Files\n",
    "\n",
    "You can also embed thresholds directly in your canary config YAML. This keeps thresholds with the test definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config with embedded thresholds\n",
    "config_with_thresholds = \"\"\"\n",
    "name: dpo_strict\n",
    "description: DPO test with embedded strict thresholds\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "training_type: dpo\n",
    "max_steps: 100\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 10\n",
    "\n",
    "beta: 0.1\n",
    "max_prompt_length: 64\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 512\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./canary_output\n",
    "\n",
    "# Embedded thresholds - used when comparing this run\n",
    "thresholds:\n",
    "  base_tier: perf  # Start from perf tier\n",
    "  max_step_time_increase_pct: 5.0  # Very strict on timing\n",
    "  max_mem_increase_mb: 200.0\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/dpo_strict.yaml', 'w') as f:\n",
    "    f.write(config_with_thresholds)\n",
    "\n",
    "print(\"Created configs/dpo_strict.yaml with embedded thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect embedded thresholds\n",
    "import yaml\n",
    "from canary.compare.thresholds import load_thresholds_from_config\n",
    "\n",
    "with open('configs/dpo_strict.yaml') as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "embedded = load_thresholds_from_config(config_dict)\n",
    "\n",
    "if embedded:\n",
    "    print(\"Embedded thresholds found:\")\n",
    "    print(f\"  Step time increase: {embedded.max_step_time_increase_pct}%\")\n",
    "    print(f\"  Memory increase: {embedded.max_mem_increase_mb}MB\")\n",
    "else:\n",
    "    print(\"No embedded thresholds found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Test Matrix System\n",
    "\n",
    "For running multiple canary tests systematically, RLHF Canary provides a test matrix system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the test matrix system\n",
    "from canary.matrix import (\n",
    "    TestTier,\n",
    "    TestDefinition,\n",
    "    TestMatrix,\n",
    "    DEFAULT_TEST_MATRIX,\n",
    "    get_tests_for_tier,\n",
    "    get_test_by_name,\n",
    ")\n",
    "\n",
    "print(\"Default Test Matrix\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Name':<15} {'Tier':<10} {'Config':<30} {'Timeout':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for test in DEFAULT_TEST_MATRIX:\n",
    "    print(f\"{test.name:<15} {test.tier.value:<10} {test.config_path:<30} {test.timeout_minutes:>7} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tests for specific tier\n",
    "smoke_tests = get_tests_for_tier(TestTier.SMOKE)\n",
    "\n",
    "print(f\"\\nSmoke tests ({len(smoke_tests)} tests):\")\n",
    "for test in smoke_tests:\n",
    "    print(f\"  - {test.name}: {test.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-built test matrices\n",
    "print(\"Pre-built Test Matrices\\n\")\n",
    "\n",
    "pr_matrix = TestMatrix.for_pr()\n",
    "print(f\"PR Gate Matrix: {pr_matrix.name}\")\n",
    "print(f\"  Description: {pr_matrix.description}\")\n",
    "print(f\"  Tests: {[t.name for t in pr_matrix.tests]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "nightly_matrix = TestMatrix.for_nightly()\n",
    "print(f\"Nightly Matrix: {nightly_matrix.name}\")\n",
    "print(f\"  Description: {nightly_matrix.description}\")\n",
    "print(f\"  Tests: {[t.name for t in nightly_matrix.tests]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "perf_matrix = TestMatrix.for_performance()\n",
    "print(f\"Performance Matrix: {perf_matrix.name}\")\n",
    "print(f\"  Description: {perf_matrix.description}\")\n",
    "print(f\"  Tests: {[t.name for t in perf_matrix.tests]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom test matrix\n",
    "from canary.compare.thresholds import SMOKE_THRESHOLDS\n",
    "\n",
    "custom_matrix = TestMatrix(\n",
    "    name=\"my_pr_gate\",\n",
    "    description=\"Custom PR gate with only DPO\",\n",
    "    tests=[\n",
    "        TestDefinition(\n",
    "            name=\"dpo_quick\",\n",
    "            description=\"Quick DPO validation\",\n",
    "            tier=TestTier.SMOKE,\n",
    "            config_path=\"configs/dpo_smoke.yaml\",\n",
    "            thresholds=SMOKE_THRESHOLDS,\n",
    "            timeout_minutes=10,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Custom matrix: {custom_matrix.name}\")\n",
    "print(f\"Tests: {len(custom_matrix.tests)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Environment Fingerprinting\n",
    "\n",
    "Environment fingerprinting captures hardware and software configuration for reproducible comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View environment fingerprint via CLI\n",
    "!python -m canary.cli env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access fingerprint programmatically\n",
    "from canary.collect.env_fingerprint import get_env_fingerprint, fingerprints_compatible\n",
    "\n",
    "fingerprint = get_env_fingerprint()\n",
    "\n",
    "print(\"Environment Fingerprint Object\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python: {fingerprint.python_version}\")\n",
    "print(f\"PyTorch: {fingerprint.torch_version}\")\n",
    "print(f\"CUDA: {fingerprint.cuda_version}\")\n",
    "print(f\"GPU: {fingerprint.gpu_name}\")\n",
    "print(f\"GPU Count: {fingerprint.gpu_count}\")\n",
    "print(f\"GPU Memory: {fingerprint.gpu_memory_gb}GB\")\n",
    "print(f\"Platform: {fingerprint.platform} {fingerprint.platform_version}\")\n",
    "print(f\"Transformers: {fingerprint.transformers_version}\")\n",
    "print(f\"TRL: {fingerprint.trl_version}\")\n",
    "print(f\"\\nFingerprint Hash: {fingerprint.fingerprint_hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fingerprint compatibility (simulated different environment)\n",
    "from canary.collect.env_fingerprint import EnvFingerprint\n",
    "\n",
    "# Simulate a different environment (A100 vs T4)\n",
    "a100_fingerprint = EnvFingerprint(\n",
    "    python_version=\"3.10.12\",\n",
    "    torch_version=\"2.1.0\",\n",
    "    cuda_available=True,\n",
    "    cuda_version=\"12.1\",\n",
    "    gpu_name=\"NVIDIA A100-SXM4-40GB\",\n",
    "    gpu_count=1,\n",
    "    gpu_memory_gb=40.0,\n",
    "    platform=\"Linux\",\n",
    "    platform_version=\"5.15.0\",\n",
    "    transformers_version=\"4.44.2\",\n",
    "    trl_version=\"0.11.4\",\n",
    ")\n",
    "\n",
    "# Check compatibility\n",
    "compatible, warnings = fingerprints_compatible(fingerprint, a100_fingerprint)\n",
    "\n",
    "print(f\"Compatible: {compatible}\")\n",
    "if warnings:\n",
    "    print(\"Warnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"  - {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Fingerprinting Matters\n",
    "\n",
    "Performance comparisons are only meaningful on similar hardware:\n",
    "\n",
    "| Scenario | Compatible | Notes |\n",
    "|----------|------------|-------|\n",
    "| Same GPU, same CUDA | Yes | Ideal for regression detection |\n",
    "| Same GPU, different CUDA | Warn | Minor perf differences possible |\n",
    "| Different GPU | No | Performance not comparable |\n",
    "| Different GPU count | No | Distributed training changes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. JSON Output for Automation\n",
    "\n",
    "For integration with dashboards, alerting, and automation, use JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get JSON output from compare command\n",
    "!python -m canary.cli compare {baseline_path} baselines/main.json --threshold-tier smoke --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON for custom processing\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"-m\", \"canary.cli\", \"compare\", str(baseline_path), \"baselines/main.json\", \"--threshold-tier\", \"smoke\", \"--json\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "report = json.loads(result.stdout)\n",
    "\n",
    "print(\"Parsed Comparison Report\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Passed: {report['passed']}\")\n",
    "print(f\"Total checks: {len(report['checks'])}\")\n",
    "print(f\"Failed checks: {len([c for c in report['checks'] if not c['passed']])}\")\n",
    "\n",
    "print(\"\\nPerformance Delta:\")\n",
    "for key, value in report['perf_delta'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nAll Checks:\")\n",
    "for check in report['checks']:\n",
    "    status = \"PASS\" if check['passed'] else \"FAIL\"\n",
    "    print(f\"  [{status}] {check['name']}: {check['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Build a dashboard payload\n",
    "def build_dashboard_payload(report: dict, metrics_path: str) -> dict:\n",
    "    \"\"\"Build a payload for a metrics dashboard.\"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    with open(metrics_path) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    return {\n",
    "        \"timestamp\": metrics.get(\"run_id\", \"\").split(\"_\")[1] if \"_\" in metrics.get(\"run_id\", \"\") else None,\n",
    "        \"run_id\": metrics.get(\"run_id\"),\n",
    "        \"passed\": report[\"passed\"],\n",
    "        \"metrics\": {\n",
    "            \"step_time_ms\": metrics[\"perf\"][\"step_time\"][\"mean\"] * 1000,\n",
    "            \"tokens_per_sec\": metrics[\"perf\"][\"approx_tokens_per_sec\"],\n",
    "            \"peak_memory_mb\": metrics[\"perf\"][\"max_mem_mb\"],\n",
    "            \"nan_steps\": metrics[\"stability\"][\"nan_steps\"],\n",
    "        },\n",
    "        \"checks\": {\n",
    "            \"total\": len(report[\"checks\"]),\n",
    "            \"passed\": sum(1 for c in report[\"checks\"] if c[\"passed\"]),\n",
    "            \"failed\": sum(1 for c in report[\"checks\"] if not c[\"passed\"]),\n",
    "        },\n",
    "        \"deltas\": report[\"perf_delta\"],\n",
    "    }\n",
    "\n",
    "payload = build_dashboard_payload(report, str(baseline_path))\n",
    "print(\"Dashboard Payload:\")\n",
    "print(json.dumps(payload, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practical Example: Multi-Tier Testing Strategy\n",
    "\n",
    "Here's how to set up a complete testing pyramid for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a testing pyramid structure\n",
    "!mkdir -p my_project/configs\n",
    "!mkdir -p my_project/thresholds\n",
    "!mkdir -p my_project/baselines\n",
    "\n",
    "# Generate configs for each tier\n",
    "!python -m canary.cli init-config my_project/configs/smoke.yaml --type dpo --tier smoke\n",
    "!python -m canary.cli init-config my_project/configs/perf.yaml --type dpo --tier perf\n",
    "!python -m canary.cli init-config my_project/configs/nightly.yaml --type dpo --tier nightly\n",
    "\n",
    "print(\"Created testing pyramid configs!\")\n",
    "!ls -la my_project/configs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tier-specific threshold files\n",
    "pr_thresholds = \"\"\"\n",
    "# PR Gate Thresholds - lenient for quick feedback\n",
    "base_tier: smoke\n",
    "max_step_time_increase_pct: 20.0\n",
    "min_step_count: 10\n",
    "\"\"\"\n",
    "\n",
    "nightly_thresholds = \"\"\"\n",
    "# Nightly Thresholds - strict for comprehensive testing\n",
    "base_tier: nightly\n",
    "max_step_time_increase_pct: 3.0\n",
    "max_tps_drop_pct: 2.0\n",
    "min_step_count: 200\n",
    "\"\"\"\n",
    "\n",
    "with open('my_project/thresholds/pr.yaml', 'w') as f:\n",
    "    f.write(pr_thresholds)\n",
    "\n",
    "with open('my_project/thresholds/nightly.yaml', 'w') as f:\n",
    "    f.write(nightly_thresholds)\n",
    "\n",
    "print(\"Created threshold files!\")\n",
    "!ls -la my_project/thresholds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the complete project structure\n",
    "!find my_project -type f | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Testing Strategy\n",
    "\n",
    "```\n",
    "my_project/\n",
    "├── configs/\n",
    "│   ├── smoke.yaml      # 100 steps, ~5-10 min\n",
    "│   ├── perf.yaml       # 500 steps, ~20-45 min\n",
    "│   └── nightly.yaml    # 2000 steps, ~1-3 hr\n",
    "├── thresholds/\n",
    "│   ├── pr.yaml         # Lenient for PR gating\n",
    "│   └── nightly.yaml    # Strict for nightly\n",
    "└── baselines/\n",
    "    └── main.json       # Current main branch baseline\n",
    "```\n",
    "\n",
    "**Workflow:**\n",
    "1. **PR opened** → Run smoke test with pr.yaml thresholds\n",
    "2. **PR merged to main** → Update baseline\n",
    "3. **Nightly (2 AM)** → Run nightly test with nightly.yaml thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **`init-config`** generates starter configs for any training type and tier\n",
    "2. **Four threshold tiers** (smoke → nightly) provide increasing strictness\n",
    "3. **Custom threshold files** allow project-specific tuning\n",
    "4. **Embedded thresholds** keep config and thresholds together\n",
    "5. **Test matrices** organize multiple tests for CI/CD\n",
    "6. **Environment fingerprints** ensure hardware-compatible comparisons\n",
    "7. **JSON output** enables dashboard and automation integration\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```bash\n",
    "# Generate config\n",
    "canary init-config output.yaml --type dpo --tier smoke\n",
    "\n",
    "# Compare with tier\n",
    "canary compare current.json baseline.json --threshold-tier perf\n",
    "\n",
    "# Compare with custom thresholds\n",
    "canary compare current.json baseline.json --threshold-file custom.yaml\n",
    "\n",
    "# JSON output\n",
    "canary compare current.json baseline.json --json\n",
    "\n",
    "# Environment info\n",
    "canary env\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [01_quickstart.ipynb](01_quickstart.ipynb) - Core workflow basics\n",
    "- [07_ci_cd_integration.ipynb](07_ci_cd_integration.ipynb) - GitHub integration\n",
    "- [09_quantization_and_memory.ipynb](09_quantization_and_memory.ipynb) - Memory optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}