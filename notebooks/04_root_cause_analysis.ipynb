{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mmcmanus1/rlhf-canary/blob/main/notebooks/04_root_cause_analysis.ipynb)\n\n# Root Cause Analysis: Debugging Regressions\n\nWhen canary tests fail, the heuristics system helps you understand why. Learn to interpret suspect rankings, match patterns to common issues, and create clear debug reports for your team.\n\n**What you'll learn:**\n1. How the heuristics system works\n2. Regression categories: dataloader, memory, kernel, IO, etc.\n3. Simulating different regression types\n4. Interpreting suspect ranking and evidence\n5. Creating a \"debug story\" for failed runs\n\n**Requirements:** GPU runtime (Runtime > Change runtime type > T4 GPU)\n\n**Runtime:** ~10-12 minutes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport sys\n\nprint(\"Starting Environment Setup...\")\n\n# --- 1. Clone or update the repo ---\nif not os.path.exists(\"/content/rlhf-canary\"):\n    !git clone https://github.com/mmcmanus1/rlhf-canary.git /content/rlhf-canary\nelse:\n    !cd /content/rlhf-canary && git pull --ff-only\n\n%cd /content/rlhf-canary\n\n# --- 2. Force-Install the \"Safe Harbor\" Stack ---\n!pip install \"trl==0.11.4\" \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"tokenizers==0.19.1\" --force-reinstall --no-deps --quiet\n!pip install -q datasets pydantic click PyYAML bitsandbytes\nprint(\"Libraries installed (TRL 0.11.4 / Transformers 4.44.2)\")\n\n# --- 3. Patch pyproject.toml (Prevent future drift) ---\nproject_file = \"/content/rlhf-canary/pyproject.toml\"\nif os.path.exists(project_file):\n    with open(project_file, \"r\") as f:\n        content = f.read()\n    \n    if \"trl==0.11.4\" not in content:\n        content = re.sub(r'trl[<>=!~]+[\\d\\.]+', 'trl==0.11.4', content)\n        with open(project_file, \"w\") as f:\n            f.write(content)\n        print(\"Config file patched to lock TRL 0.11.4\")\n\n# --- 4. Patch Source Code (Compatibility Fix) ---\nrunner_file = \"/content/rlhf-canary/canary/runner/local.py\"\nif os.path.exists(runner_file):\n    with open(runner_file, \"r\") as f:\n        code = f.read()\n    \n    if \"processing_class=\" in code:\n        code = code.replace(\"processing_class=\", \"tokenizer=\")\n        with open(runner_file, \"w\") as f:\n            f.write(code)\n        print(\"Code patched: Reverted 'processing_class' to 'tokenizer'\")\n    else:\n        print(\"Code is already compatible.\")\n\n# --- 5. Install the package ---\n!pip install -e . --quiet\n\nprint(\"Environment Ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU and installation\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "import canary\n",
    "print(f\"Canary module loaded from: {canary.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Heuristics System\n",
    "\n",
    "When a canary run shows regressions, the heuristics system analyzes the pattern of failed checks to suggest likely root causes.\n",
    "\n",
    "### Regression Categories\n",
    "\n",
    "| Category | What it indicates | Common causes |\n",
    "|----------|------------------|---------------|\n",
    "| `DATALOADER` | CPU/IO bottleneck | num_workers, preprocessing |\n",
    "| `TOKENIZATION` | Tokenizer changes | New tokenizer, different config |\n",
    "| `COMMUNICATION` | Multi-GPU issues | NCCL, DDP changes |\n",
    "| `MEMORY` | Memory pressure | Leaks, fragmentation |\n",
    "| `KERNEL` | GPU kernel changes | Model architecture, batch size |\n",
    "| `IO` | I/O overhead | Checkpointing, logging |\n",
    "| `UNKNOWN` | Unclear cause | Needs manual investigation |\n",
    "\n",
    "### How Suspects Are Ranked\n",
    "\n",
    "Each suspect has:\n",
    "- **Confidence score** (0.0 - 1.0): How likely this is the cause\n",
    "- **Evidence**: Specific metrics that support this diagnosis\n",
    "- **Suggested actions**: Concrete steps to fix the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the regression categories from the code\n",
    "from canary.compare.heuristics import RegressionCategory\n",
    "\n",
    "print(\"Regression Categories:\")\n",
    "print(\"=\"*40)\n",
    "for cat in RegressionCategory:\n",
    "    print(f\"  {cat.value:<15} - {cat.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Baseline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline DPO canary\n",
    "!python -m canary.cli run configs/dpo_smoke.yaml -o ./rca_output/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\n# Load baseline metrics\nbaseline_paths = list(Path('./rca_output/baseline').rglob('metrics.json'))\nif not baseline_paths:\n    raise FileNotFoundError(\"No metrics.json found for baseline. Did the training complete?\")\n\nbaseline_path = baseline_paths[0]\nwith open(baseline_path) as f:\n    baseline_metrics = json.load(f)\n\nprint(\"Baseline Metrics:\")\nprint(f\"  Step time (mean): {baseline_metrics['perf']['step_time']['mean']:.4f}s\")\nprint(f\"  Tokens/sec: {baseline_metrics['perf']['approx_tokens_per_sec']:.0f}\")\nprint(f\"  Peak memory: {baseline_metrics['perf']['max_mem_mb']:.0f}MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulate a Dataloader Bottleneck\n",
    "\n",
    "A common regression is when the dataloader becomes a bottleneck. This typically shows as:\n",
    "- Step time increases\n",
    "- GPU utilization drops\n",
    "- Memory stays stable\n",
    "\n",
    "We'll simulate this by reducing the batch size (which increases Python overhead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config that simulates dataloader bottleneck\n",
    "dataloader_bottleneck_config = \"\"\"\n",
    "name: dpo_dataloader_bottleneck\n",
    "description: Simulates dataloader bottleneck via small batch\n",
    "\n",
    "# Model configuration\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# Training configuration - SMALL BATCH = MORE DATALOADER CALLS\n",
    "training_type: dpo\n",
    "max_steps: 60\n",
    "batch_size: 1              # Tiny batch = high Python overhead\n",
    "gradient_accumulation_steps: 8\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 5\n",
    "\n",
    "# DPO-specific\n",
    "beta: 0.1\n",
    "max_prompt_length: 64\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 256\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./rca_output\n",
    "metrics_warmup_steps: 5\n",
    "\n",
    "profiler:\n",
    "  enabled: false\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/dpo_dataloader_bottleneck.yaml', 'w') as f:\n",
    "    f.write(dataloader_bottleneck_config)\n",
    "\n",
    "print(\"Created config with batch_size=1 to simulate dataloader bottleneck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dataloader bottleneck config\n",
    "!python -m canary.cli run configs/dpo_dataloader_bottleneck.yaml -o ./rca_output/dataloader_bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Root Cause Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the regression metrics\nregression_paths = list(Path('./rca_output/dataloader_bottleneck').rglob('metrics.json'))\nif not regression_paths:\n    raise FileNotFoundError(\"No metrics.json found for regression run. Did the training complete?\")\n\nregression_path = regression_paths[0]\nwith open(regression_path) as f:\n    regression_metrics = json.load(f)\n\nprint(\"Regression Metrics:\")\nprint(f\"  Step time (mean): {regression_metrics['perf']['step_time']['mean']:.4f}s\")\nprint(f\"  Tokens/sec: {regression_metrics['perf']['approx_tokens_per_sec']:.0f}\")\nprint(f\"  Peak memory: {regression_metrics['perf']['max_mem_mb']:.0f}MB\")\n\n# Calculate deltas\nbase_step = baseline_metrics['perf']['step_time']['mean']\nreg_step = regression_metrics['perf']['step_time']['mean']\nstep_delta = (reg_step - base_step) / base_step * 100\n\nbase_tps = baseline_metrics['perf']['approx_tokens_per_sec']\nreg_tps = regression_metrics['perf']['approx_tokens_per_sec']\ntps_delta = (reg_tps - base_tps) / base_tps * 100\n\nbase_mem = baseline_metrics['perf']['max_mem_mb']\nreg_mem = regression_metrics['perf']['max_mem_mb']\nmem_delta = reg_mem - base_mem\n\nprint(f\"\\nDeltas:\")\nprint(f\"  Step time: {step_delta:+.1f}%\")\nprint(f\"  Tokens/sec: {tps_delta:+.1f}%\")\nprint(f\"  Memory: {mem_delta:+.0f}MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison with root cause analysis\n",
    "from canary.compare.stats import compare_to_baseline, load_metrics\n",
    "from canary.compare.thresholds import SMOKE_THRESHOLDS\n",
    "from canary.compare.heuristics import analyze_regression, format_suspects_markdown\n",
    "\n",
    "# Load metrics properly\n",
    "current = load_metrics(str(regression_path))\n",
    "baseline = load_metrics(str(baseline_path))\n",
    "\n",
    "# Run comparison\n",
    "report = compare_to_baseline(current, baseline, SMOKE_THRESHOLDS)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOverall: {'PASS' if report.passed else 'FAIL'}\")\n",
    "print(f\"\\nFailed checks ({len(report.failed_checks)}):\")\n",
    "for check in report.failed_checks:\n",
    "    delta_str = f\"{check.delta_pct:+.1f}%\" if check.delta_pct else f\"{check.delta:+.1f}\"\n",
    "    print(f\"  - {check.name}: {delta_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run root cause analysis\n",
    "analysis = analyze_regression(report, current, baseline)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSummary: {analysis.summary}\")\n",
    "print(f\"\\nTop {len(analysis.suspects)} suspects:\")\n",
    "\n",
    "for i, suspect in enumerate(analysis.suspects, 1):\n",
    "    conf_bar = \"█\" * int(suspect.confidence * 10) + \"░\" * (10 - int(suspect.confidence * 10))\n",
    "    print(f\"\\n#{i} {suspect.category.value.upper()} [{conf_bar}] {suspect.confidence:.0%}\")\n",
    "    print(f\"   {suspect.description}\")\n",
    "    print(f\"   Evidence:\")\n",
    "    for ev in suspect.evidence:\n",
    "        print(f\"     - {ev}\")\n",
    "    print(f\"   Actions:\")\n",
    "    for action in suspect.suggested_actions[:2]:  # Show top 2 actions\n",
    "        print(f\"     - {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simulate a Memory Regression\n",
    "\n",
    "Memory regressions show as:\n",
    "- Memory increases significantly\n",
    "- Throughput may drop due to memory pressure\n",
    "\n",
    "We'll simulate this by increasing the sequence length (which uses more memory for activations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config with higher memory usage\n",
    "memory_regression_config = \"\"\"\n",
    "name: dpo_memory_regression\n",
    "description: Simulates memory regression via longer sequences\n",
    "\n",
    "# Model configuration\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "# Training configuration - LONGER SEQUENCES = MORE MEMORY\n",
    "training_type: dpo\n",
    "max_steps: 60\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 512            # 2x longer sequences!\n",
    "warmup_steps: 5\n",
    "\n",
    "# DPO-specific\n",
    "beta: 0.1\n",
    "max_prompt_length: 128     # Also longer prompts\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 256\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./rca_output\n",
    "metrics_warmup_steps: 5\n",
    "\n",
    "profiler:\n",
    "  enabled: false\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/dpo_memory_regression.yaml', 'w') as f:\n",
    "    f.write(memory_regression_config)\n",
    "\n",
    "print(\"Created config with max_length=512 (2x baseline) to simulate memory regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the memory regression config\n",
    "!python -m canary.cli run configs/dpo_memory_regression.yaml -o ./rca_output/memory_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze the memory regression\nmem_reg_paths = list(Path('./rca_output/memory_regression').rglob('metrics.json'))\nif not mem_reg_paths:\n    raise FileNotFoundError(\"No metrics.json found for memory regression run. Did the training complete?\")\n\nmem_reg_path = mem_reg_paths[0]\nmem_current = load_metrics(str(mem_reg_path))\n\n# Run comparison\nmem_report = compare_to_baseline(mem_current, baseline, SMOKE_THRESHOLDS)\nmem_analysis = analyze_regression(mem_report, mem_current, baseline)\n\nprint(\"=\"*60)\nprint(\"MEMORY REGRESSION ANALYSIS\")\nprint(\"=\"*60)\nprint(f\"\\nOverall: {'PASS' if mem_report.passed else 'FAIL'}\")\n\nif mem_report.failed_checks:\n    print(f\"\\nFailed checks:\")\n    for check in mem_report.failed_checks:\n        delta_str = f\"{check.delta_pct:+.1f}%\" if check.delta_pct else f\"{check.delta:+.1f}\"\n        print(f\"  - {check.name}: {delta_str}\")\n\nprint(f\"\\nRoot cause summary: {mem_analysis.summary}\")\nif mem_analysis.top_suspect:\n    print(f\"\\nTop suspect: {mem_analysis.top_suspect.category.value}\")\n    print(f\"Confidence: {mem_analysis.top_suspect.confidence:.0%}\")\n    print(f\"Description: {mem_analysis.top_suspect.description}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using the CLI for Root Cause Analysis\n",
    "\n",
    "You can also get root cause analysis directly from the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CLI to compare and get analysis\n",
    "!python -m canary.cli compare {regression_path} {baseline_path} --threshold-tier smoke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating a Debug Story\n",
    "\n",
    "When presenting a regression to your team, you want a clear \"debug story\". Here's how to structure it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_debug_story(report, analysis, current, baseline):\n    \"\"\"Create a structured debug story from canary results.\"\"\"\n    \n    story = []\n    story.append(\"# Regression Debug Report\")\n    story.append(\"\")\n    \n    # 1. What happened\n    story.append(\"## 1. What Happened\")\n    story.append(f\"- Canary result: **{'PASS' if report.passed else 'FAIL'}**\")\n    story.append(f\"- Failed checks: {len(report.failed_checks)}\")\n    for check in report.failed_checks:\n        delta_str = f\"{check.delta_pct:+.1f}%\" if check.delta_pct else f\"{check.delta:+.1f}\"\n        story.append(f\"  - {check.name}: {delta_str}\")\n    story.append(\"\")\n    \n    # 2. Root cause\n    story.append(\"## 2. Root Cause Analysis\")\n    story.append(f\"**Summary:** {analysis.summary}\")\n    story.append(\"\")\n    \n    if analysis.top_suspect:\n        ts = analysis.top_suspect\n        story.append(f\"**Primary suspect:** {ts.category.value} ({ts.confidence:.0%} confidence)\")\n        story.append(f\"\")\n        story.append(\"Evidence:\")\n        for ev in ts.evidence:\n            story.append(f\"- {ev}\")\n    story.append(\"\")\n    \n    # 3. Recommended actions\n    story.append(\"## 3. Recommended Actions\")\n    if analysis.top_suspect:\n        for i, action in enumerate(analysis.top_suspect.suggested_actions, 1):\n            story.append(f\"{i}. {action}\")\n    story.append(\"\")\n    \n    # 4. Metrics comparison\n    story.append(\"## 4. Metrics Comparison\")\n    story.append(\"| Metric | Baseline | Current | Delta |\")\n    story.append(\"|--------|----------|---------|-------|\")\n    \n    base_step = baseline.perf.step_time.mean\n    curr_step = current.perf.step_time.mean\n    if base_step is not None and base_step > 0 and curr_step is not None:\n        step_delta = (curr_step - base_step) / base_step * 100\n        story.append(f\"| Step time | {base_step:.4f}s | {curr_step:.4f}s | {step_delta:+.1f}% |\")\n    else:\n        story.append(f\"| Step time | N/A | N/A | N/A |\")\n    \n    base_tps = baseline.perf.approx_tokens_per_sec\n    curr_tps = current.perf.approx_tokens_per_sec\n    if base_tps is not None and base_tps > 0 and curr_tps is not None:\n        tps_delta = (curr_tps - base_tps) / base_tps * 100\n        story.append(f\"| Tokens/sec | {base_tps:.0f} | {curr_tps:.0f} | {tps_delta:+.1f}% |\")\n    else:\n        story.append(f\"| Tokens/sec | N/A | N/A | N/A |\")\n    \n    base_mem = baseline.perf.max_mem_mb\n    curr_mem = current.perf.max_mem_mb\n    if base_mem is not None and curr_mem is not None:\n        mem_delta = curr_mem - base_mem\n        story.append(f\"| Memory | {base_mem:.0f}MB | {curr_mem:.0f}MB | {mem_delta:+.0f}MB |\")\n    else:\n        story.append(f\"| Memory | N/A | N/A | N/A |\")\n    \n    return \"\\n\".join(story)\n\n# Generate debug story for our dataloader regression\ndebug_story = create_debug_story(report, analysis, current, baseline)\nprint(debug_story)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pattern Recognition Guide\n",
    "\n",
    "Here's a quick reference for recognizing regression patterns:\n",
    "\n",
    "| Pattern | Step Time | Memory | GPU Util | Likely Cause |\n",
    "|---------|-----------|--------|----------|-------------|\n",
    "| Step time ↑, Memory stable | ↑ | ~ | ↓ | Dataloader/CPU |\n",
    "| Step time ↑, Memory ↑ | ↑ | ↑ | ~ | Memory fragmentation |\n",
    "| Memory ↑↑, Step time ~ | ~ | ↑↑ | ~ | Memory leak |\n",
    "| All metrics worse | ↑ | ↑ | ↓ | Major regression |\n",
    "| NaN detected | N/A | N/A | N/A | Numerical instability |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick pattern matcher\n",
    "def match_regression_pattern(report, current, baseline):\n",
    "    \"\"\"Match regression to common patterns.\"\"\"\n",
    "    \n",
    "    failed_names = {c.name for c in report.failed_checks}\n",
    "    \n",
    "    # Check for NaN/Inf first (highest priority)\n",
    "    if 'nan_steps' in failed_names or 'inf_steps' in failed_names:\n",
    "        return \"NUMERICAL_INSTABILITY\", \"Check learning rate, gradient clipping, data preprocessing\"\n",
    "    \n",
    "    # Pattern: Step time up, memory stable\n",
    "    if 'step_time_mean' in failed_names and 'max_memory' not in failed_names:\n",
    "        return \"CPU_BOTTLENECK\", \"Likely dataloader, tokenization, or Python overhead\"\n",
    "    \n",
    "    # Pattern: Step time up, memory up\n",
    "    if 'step_time_mean' in failed_names and 'max_memory' in failed_names:\n",
    "        return \"MEMORY_FRAGMENTATION\", \"Likely CUDA allocator issue or memory leak\"\n",
    "    \n",
    "    # Pattern: Only memory up\n",
    "    if 'max_memory' in failed_names and 'step_time_mean' not in failed_names:\n",
    "        return \"MEMORY_INCREASE\", \"Model size, batch size, or sequence length change\"\n",
    "    \n",
    "    # Pattern: Only throughput down\n",
    "    if 'tokens_per_sec' in failed_names:\n",
    "        return \"THROUGHPUT_DROP\", \"Check batch efficiency, model changes\"\n",
    "    \n",
    "    return \"UNKNOWN\", \"Requires manual investigation\"\n",
    "\n",
    "# Test pattern matcher on our regressions\n",
    "print(\"Pattern Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pattern, suggestion = match_regression_pattern(report, current, baseline)\n",
    "print(f\"\\nDataloader bottleneck run:\")\n",
    "print(f\"  Pattern: {pattern}\")\n",
    "print(f\"  Suggestion: {suggestion}\")\n",
    "\n",
    "if mem_report.failed_checks:\n",
    "    pattern2, suggestion2 = match_regression_pattern(mem_report, mem_current, baseline)\n",
    "    print(f\"\\nMemory regression run:\")\n",
    "    print(f\"  Pattern: {pattern2}\")\n",
    "    print(f\"  Suggestion: {suggestion2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Heuristics analyze patterns** of failed checks to suggest root causes\n",
    "2. **Suspects are ranked** by confidence score based on evidence\n",
    "3. **Common patterns** map to specific categories (dataloader, memory, etc.)\n",
    "4. **Each suspect includes** evidence and suggested actions\n",
    "5. **Debug stories** help communicate regressions to your team\n",
    "\n",
    "### When to Use Root Cause Analysis:\n",
    "\n",
    "- After any canary failure\n",
    "- When investigating performance degradation\n",
    "- When onboarding new team members to debugging\n",
    "- When creating post-mortems for incidents\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- See `05_ppo_canary.ipynb` for PPO-specific canary workflows\n",
    "- See `02_profiler_deep_dive.ipynb` for deeper performance analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}