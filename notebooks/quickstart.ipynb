{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RLHF Canary - Quickstart\n",
        "\n",
        "This notebook demonstrates how to use RLHF Canary to detect regressions in your RLHF/finetuning pipelines.\n",
        "\n",
        "**What you'll learn:**\n",
        "1. Run a DPO canary training job\n",
        "2. Save metrics as a baseline\n",
        "3. Compare new runs against baseline\n",
        "4. Interpret regression reports\n",
        "\n",
        "**Requirements:** GPU runtime (Runtime > Change runtime type > T4 GPU)"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets accelerate trl peft bitsandbytes\n",
        "!pip install -q pydantic click PyYAML\n",
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/mmcmanus1/rlhf-canary.git\n",
        "%cd rlhf-canary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GPU is available\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "check-gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show environment fingerprint\n",
        "!python -m canary.cli env"
      ],
      "metadata": {
        "id": "env-fingerprint"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Run Baseline Canary"
      ],
      "metadata": {
        "id": "baseline-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run DPO smoke test (takes ~5-10 min on T4)\n",
        "!python -m canary.cli run configs/dpo_smoke.yaml -o ./canary_output"
      ],
      "metadata": {
        "id": "run-baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find and display the metrics\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "metrics_files = list(Path('./canary_output').rglob('metrics.json'))\n",
        "if metrics_files:\n",
        "    baseline_path = metrics_files[0]\n",
        "    print(f\"Baseline metrics: {baseline_path}\")\n",
        "    \n",
        "    with open(baseline_path) as f:\n",
        "        metrics = json.load(f)\n",
        "    \n",
        "    print(f\"\\nRun ID: {metrics['run_id']}\")\n",
        "    print(f\"Duration: {metrics['duration_seconds']:.1f}s\")\n",
        "    print(f\"Step time (mean): {metrics['perf']['step_time']['mean']:.4f}s\")\n",
        "    print(f\"Tokens/sec: {metrics['perf']['approx_tokens_per_sec']:.0f}\")\n",
        "    print(f\"Peak memory: {metrics['perf']['max_mem_mb']:.0f}MB\")\n",
        "    print(f\"NaN steps: {metrics['stability']['nan_steps']}\")"
      ],
      "metadata": {
        "id": "show-baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as baseline\n",
        "!mkdir -p baselines\n",
        "!cp {baseline_path} baselines/dpo_baseline.json\n",
        "print(\"Baseline saved to baselines/dpo_baseline.json\")"
      ],
      "metadata": {
        "id": "save-baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Run Again and Compare\n",
        "\n",
        "Now let's run another canary and compare it to our baseline."
      ],
      "metadata": {
        "id": "compare-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run another canary\n",
        "!python -m canary.cli run configs/dpo_smoke.yaml -o ./canary_output"
      ],
      "metadata": {
        "id": "run-current"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the new metrics file\n",
        "metrics_files = sorted(Path('./canary_output').rglob('metrics.json'), key=lambda p: p.stat().st_mtime)\n",
        "current_path = metrics_files[-1]  # Most recent\n",
        "print(f\"Current metrics: {current_path}\")"
      ],
      "metadata": {
        "id": "find-current"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare to baseline\n",
        "!python -m canary.cli compare {current_path} baselines/dpo_baseline.json --threshold-tier smoke"
      ],
      "metadata": {
        "id": "compare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Simulate a Regression\n",
        "\n",
        "Let's intentionally create a slower config to see regression detection in action."
      ],
      "metadata": {
        "id": "regression-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a \"slower\" config (smaller batch = more steps = slower)\n",
        "slow_config = \"\"\"\n",
        "name: dpo_slow\n",
        "description: Intentionally slow config for regression demo\n",
        "\n",
        "model_name: EleutherAI/pythia-70m\n",
        "use_peft: true\n",
        "lora_r: 16\n",
        "lora_alpha: 32\n",
        "lora_dropout: 0.05\n",
        "\n",
        "training_type: dpo\n",
        "max_steps: 100\n",
        "batch_size: 1  # Smaller batch = slower!\n",
        "gradient_accumulation_steps: 8\n",
        "learning_rate: 5.0e-5\n",
        "max_length: 256\n",
        "warmup_steps: 10\n",
        "\n",
        "beta: 0.1\n",
        "max_prompt_length: 64\n",
        "\n",
        "dataset_name: Anthropic/hh-rlhf\n",
        "dataset_split: train\n",
        "dataset_size: 512\n",
        "seed: 42\n",
        "\n",
        "output_dir: ./canary_output\n",
        "metrics_warmup_steps: 10\n",
        "\"\"\"\n",
        "\n",
        "with open('configs/dpo_slow.yaml', 'w') as f:\n",
        "    f.write(slow_config)\n",
        "\n",
        "print(\"Created slow config\")"
      ],
      "metadata": {
        "id": "create-slow-config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the slow config\n",
        "!python -m canary.cli run configs/dpo_slow.yaml -o ./canary_output"
      ],
      "metadata": {
        "id": "run-slow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare slow run to baseline - should show regression!\n",
        "metrics_files = sorted(Path('./canary_output').rglob('metrics.json'), key=lambda p: p.stat().st_mtime)\n",
        "slow_path = metrics_files[-1]\n",
        "\n",
        "!python -m canary.cli compare {slow_path} baselines/dpo_baseline.json --threshold-tier smoke"
      ],
      "metadata": {
        "id": "compare-slow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Next Steps\n",
        "\n",
        "- **PR Gating**: Add the GitHub Actions workflow to your repo\n",
        "- **Nightly Tests**: Run longer `dpo_perf.yaml` tests overnight\n",
        "- **Custom Thresholds**: Adjust thresholds for your use case\n",
        "- **Root Cause Analysis**: Check the heuristics when regressions occur\n",
        "\n",
        "See the [README](https://github.com/mmcmanus1/rlhf-canary) for more details."
      ],
      "metadata": {
        "id": "next-steps"
      }
    }
  ]
}
