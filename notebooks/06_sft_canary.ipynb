{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT Canary: Supervised Fine-Tuning Validation\n",
    "\n",
    "This notebook demonstrates how to use RLHF Canary for SFT (Supervised Fine-Tuning) validation.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. SFT vs DPO vs PPO: When to use each\n",
    "2. Running SFT canaries\n",
    "3. SFT-specific characteristics and metrics\n",
    "4. Memory efficiency of SFT\n",
    "5. Comparing SFT baselines\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime > Change runtime type > T4 GPU)\n",
    "\n",
    "**Runtime:** ~8-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "print(\"Starting Environment Setup...\")\n",
    "\n",
    "# --- 1. Clone the repo first ---\n",
    "if not os.path.exists(\"/content/rlhf-canary\"):\n",
    "    !git clone https://github.com/mmcmanus1/rlhf-canary.git /content/rlhf-canary\n",
    "\n",
    "%cd /content/rlhf-canary\n",
    "\n",
    "# --- 2. Force-Install the \"Safe Harbor\" Stack ---\n",
    "!pip install \"trl==0.11.4\" \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"tokenizers==0.19.1\" --force-reinstall --no-deps --quiet\n",
    "!pip install -q datasets pydantic click PyYAML bitsandbytes\n",
    "print(\"Libraries installed (TRL 0.11.4 / Transformers 4.44.2)\")\n",
    "\n",
    "# --- 3. Patch pyproject.toml (Prevent future drift) ---\n",
    "project_file = \"/content/rlhf-canary/pyproject.toml\"\n",
    "if os.path.exists(project_file):\n",
    "    with open(project_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    if \"trl==0.11.4\" not in content:\n",
    "        content = re.sub(r'trl[<>=!~]+[\\d\\.]+', 'trl==0.11.4', content)\n",
    "        with open(project_file, \"w\") as f:\n",
    "            f.write(content)\n",
    "        print(\"Config file patched to lock TRL 0.11.4\")\n",
    "\n",
    "# --- 4. Patch Source Code (Compatibility Fix) ---\n",
    "runner_file = \"/content/rlhf-canary/canary/runner/local.py\"\n",
    "if os.path.exists(runner_file):\n",
    "    with open(runner_file, \"r\") as f:\n",
    "        code = f.read()\n",
    "    \n",
    "    if \"processing_class=\" in code:\n",
    "        code = code.replace(\"processing_class=\", \"tokenizer=\")\n",
    "        with open(runner_file, \"w\") as f:\n",
    "            f.write(code)\n",
    "        print(\"Code patched: Reverted 'processing_class' to 'tokenizer'\")\n",
    "    else:\n",
    "        print(\"Code is already compatible.\")\n",
    "\n",
    "# --- 5. Install the package ---\n",
    "!pip install -e . --quiet\n",
    "\n",
    "print(\"Environment Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU and installation\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "import canary\n",
    "print(f\"Canary module loaded from: {canary.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding SFT vs DPO vs PPO\n",
    "\n",
    "### Training Methods Comparison\n",
    "\n",
    "| Aspect | SFT | DPO | PPO |\n",
    "|--------|-----|-----|-----|\n",
    "| **Goal** | Imitate good responses | Learn from preferences | Optimize reward |\n",
    "| **Data** | Text completions | Preference pairs | Prompts + reward model |\n",
    "| **Memory** | ~1x model | ~2x model | ~3x model |\n",
    "| **Complexity** | Simple | Medium | Complex |\n",
    "| **Speed** | Fastest | Medium | Slowest |\n",
    "\n",
    "### When to Use SFT Canaries\n",
    "\n",
    "1. **Pre-training validation**: Before running expensive DPO/PPO\n",
    "2. **Baseline comparison**: Establish performance floor\n",
    "3. **Infrastructure testing**: Verify training pipeline works\n",
    "4. **Memory-constrained**: When DPO/PPO won't fit\n",
    "5. **Quick iteration**: Fastest feedback loop\n",
    "\n",
    "### SFT Data Format\n",
    "\n",
    "```python\n",
    "# SFT uses single text sequences\n",
    "{\"text\": \"Human: What is 2+2?\\n\\nAssistant: 2+2 equals 4.\"}\n",
    "\n",
    "# vs DPO which needs preference pairs\n",
    "{\"chosen\": \"Good response\", \"rejected\": \"Bad response\"}\n",
    "\n",
    "# vs PPO which generates and scores\n",
    "{\"query\": \"Human: What is 2+2?\\n\\nAssistant:\"} → generate → score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SFT Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the SFT smoke config\n",
    "!cat configs/sft_smoke.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key differences from DPO config:\n",
    "print(\"SFT vs DPO Configuration Differences:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diffs = [\n",
    "    (\"training_type\", \"sft\", \"dpo\"),\n",
    "    (\"beta\", \"N/A (not needed)\", \"0.1 (KL penalty)\"),\n",
    "    (\"max_prompt_length\", \"N/A (not needed)\", \"64\"),\n",
    "    (\"Data usage\", \"'chosen' column only\", \"Both 'chosen' and 'rejected'\"),\n",
    "    (\"Reference model\", \"Not created\", \"Created internally (~2x memory)\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Parameter':<20} {'SFT':>20} {'DPO':>20}\")\n",
    "print(\"-\"*60)\n",
    "for param, sft_val, dpo_val in diffs:\n",
    "    print(f\"{param:<20} {sft_val:>20} {dpo_val:>20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run SFT Baseline Canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SFT smoke test (~5-8 min)\n",
    "!python -m canary.cli run configs/sft_smoke.yaml -o ./sft_output/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\n# Load and display SFT metrics\nsft_baseline_paths = list(Path('./sft_output/baseline').rglob('metrics.json'))\nif not sft_baseline_paths:\n    raise FileNotFoundError(\"No metrics.json found for SFT baseline. Did the training complete?\")\n\nsft_baseline_path = sft_baseline_paths[0]\nwith open(sft_baseline_path) as f:\n    sft_metrics = json.load(f)\n\nprint(\"=\"*60)\nprint(\"SFT BASELINE METRICS\")\nprint(\"=\"*60)\n\nprint(f\"\\nPerformance:\")\nprint(f\"  Step time (mean): {sft_metrics['perf']['step_time']['mean']:.4f}s\")\nprint(f\"  Tokens/sec: {sft_metrics['perf']['approx_tokens_per_sec']:.0f}\")\nprint(f\"  Peak memory: {sft_metrics['perf']['max_mem_mb']:.0f}MB\")\n\nprint(f\"\\nStability:\")\nprint(f\"  NaN steps: {sft_metrics['stability']['nan_steps']}\")\nprint(f\"  Inf steps: {sft_metrics['stability']['inf_steps']}\")\nprint(f\"  Loss diverged: {sft_metrics['stability']['loss_diverged']}\")\nprint(f\"  Final loss: {sft_metrics['stability']['final_loss']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory Comparison: SFT vs DPO vs PPO\n",
    "\n",
    "Let's run DPO and compare memory usage to see SFT's efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DPO for comparison\n",
    "!python -m canary.cli run configs/dpo_smoke.yaml -o ./sft_output/dpo_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load DPO metrics\ndpo_paths = list(Path('./sft_output/dpo_comparison').rglob('metrics.json'))\nif not dpo_paths:\n    raise FileNotFoundError(\"No metrics.json found for DPO comparison. Did the training complete?\")\n\ndpo_path = dpo_paths[0]\nwith open(dpo_path) as f:\n    dpo_metrics = json.load(f)\n\nprint(\"=\"*60)\nprint(\"MEMORY & PERFORMANCE COMPARISON\")\nprint(\"=\"*60)\n\nsft_mem = sft_metrics['perf']['max_mem_mb']\ndpo_mem = dpo_metrics['perf']['max_mem_mb']\n\nsft_step = sft_metrics['perf']['step_time']['mean']\ndpo_step = dpo_metrics['perf']['step_time']['mean']\n\nsft_tps = sft_metrics['perf']['approx_tokens_per_sec']\ndpo_tps = dpo_metrics['perf']['approx_tokens_per_sec']\n\n# Sanity check for valid values before computing ratios\nif sft_mem > 0 and sft_step > 0 and dpo_tps > 0:\n    print(f\"\\n{'Metric':<20} {'SFT':>15} {'DPO':>15} {'Ratio':>15}\")\n    print(\"-\"*65)\n    print(f\"{'Peak Memory (MB)':<20} {sft_mem:>15.0f} {dpo_mem:>15.0f} {dpo_mem/sft_mem:>14.1f}x\")\n    print(f\"{'Step Time (s)':<20} {sft_step:>15.4f} {dpo_step:>15.4f} {dpo_step/sft_step:>14.1f}x\")\n    print(f\"{'Tokens/sec':<20} {sft_tps:>15.0f} {dpo_tps:>15.0f} {sft_tps/dpo_tps:>14.1f}x\")\n\n    print(f\"\\nKey Insight:\")\n    print(f\"  - DPO uses {dpo_mem/sft_mem:.1f}x more memory than SFT\")\n    print(f\"  - SFT processes {sft_tps/dpo_tps:.1f}x more tokens per second\")\n    print(f\"  - This is because DPO creates an internal reference model\")\nelse:\n    print(\"Warning: Some metrics appear invalid, skipping ratio calculations\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save SFT Baseline and Compare Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SFT baseline\n",
    "!mkdir -p baselines\n",
    "!cp {sft_baseline_path} baselines/sft_baseline.json\n",
    "print(\"Saved SFT baseline to baselines/sft_baseline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run another SFT canary with different seed\n",
    "sft_run2_config = \"\"\"\n",
    "name: sft_run2\n",
    "description: Second SFT run for comparison\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "training_type: sft\n",
    "max_steps: 100\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 10\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 512\n",
    "seed: 123  # Different seed\n",
    "\n",
    "output_dir: ./sft_output\n",
    "metrics_warmup_steps: 10\n",
    "\n",
    "profiler:\n",
    "  enabled: false\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/sft_run2.yaml', 'w') as f:\n",
    "    f.write(sft_run2_config)\n",
    "\n",
    "print(\"Created second SFT config with different seed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run second SFT canary\n",
    "!python -m canary.cli run configs/sft_run2.yaml -o ./sft_output/run2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare to baseline\nrun2_paths = list(Path('./sft_output/run2').rglob('metrics.json'))\nif not run2_paths:\n    raise FileNotFoundError(\"No metrics.json found for run2. Did the training complete?\")\n\nrun2_path = run2_paths[0]\n!python -m canary.cli compare {run2_path} baselines/sft_baseline.json --threshold-tier smoke"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SFT with Profiler\n",
    "\n",
    "SFT supports profiler integration (unlike PPO's manual loop). Let's run a profiled SFT job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SFT config with profiler\n",
    "sft_profiled_config = \"\"\"\n",
    "name: sft_profiled\n",
    "description: SFT with profiler enabled\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "training_type: sft\n",
    "max_steps: 80\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 5.0e-5\n",
    "max_length: 256\n",
    "warmup_steps: 10\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 512\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./sft_output\n",
    "metrics_warmup_steps: 10\n",
    "\n",
    "# Enable profiler\n",
    "profiler:\n",
    "  enabled: true\n",
    "  start_step: 50\n",
    "  num_steps: 20\n",
    "  output_dir: ./sft_profiler_traces\n",
    "  record_shapes: true\n",
    "  profile_memory: true\n",
    "  with_stack: false\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/sft_profiled.yaml', 'w') as f:\n",
    "    f.write(sft_profiled_config)\n",
    "\n",
    "print(\"Created profiled SFT config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run profiled SFT\n",
    "!python -m canary.cli run configs/sft_profiled.yaml -o ./sft_output/profiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check profiler output\nprofiled_paths = list(Path('./sft_output/profiled').rglob('metrics.json'))\nif not profiled_paths:\n    raise FileNotFoundError(\"No metrics.json found for profiled run. Did the training complete?\")\n\nprofiled_path = profiled_paths[0]\nwith open(profiled_path) as f:\n    profiled_metrics = json.load(f)\n\nif profiled_metrics.get('profiler'):\n    prof = profiled_metrics['profiler']\n    print(\"=\"*60)\n    print(\"SFT PROFILER SUMMARY\")\n    print(\"=\"*60)\n    print(f\"\\nTotal CUDA time: {prof.get('cuda_time_total_ms', 0):.2f} ms\")\n    print(f\"Total CPU time: {prof.get('cpu_time_total_ms', 0):.2f} ms\")\n    \n    if prof.get('top_cuda_ops'):\n        print(f\"\\nTop 5 CUDA operations:\")\n        for op in prof['top_cuda_ops'][:5]:\n            print(f\"  {op['name'][:40]:<40} {op['self_cuda_time_ms']:>8.2f}ms\")\nelse:\n    print(\"No profiler data in metrics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SFT Stability Testing\n",
    "\n",
    "Let's test SFT stability detection with an intentionally unstable config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unstable SFT config\n",
    "sft_unstable_config = \"\"\"\n",
    "name: sft_unstable\n",
    "description: Intentionally unstable SFT - high learning rate\n",
    "\n",
    "model_name: EleutherAI/pythia-70m\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 32\n",
    "lora_dropout: 0.05\n",
    "\n",
    "training_type: sft\n",
    "max_steps: 50\n",
    "batch_size: 2\n",
    "gradient_accumulation_steps: 4\n",
    "learning_rate: 1.0e-2   # 200x higher than normal!\n",
    "max_length: 256\n",
    "warmup_steps: 2\n",
    "\n",
    "dataset_name: Anthropic/hh-rlhf\n",
    "dataset_split: train\n",
    "dataset_size: 256\n",
    "seed: 42\n",
    "\n",
    "output_dir: ./sft_output\n",
    "metrics_warmup_steps: 2\n",
    "\n",
    "profiler:\n",
    "  enabled: false\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/sft_unstable.yaml', 'w') as f:\n",
    "    f.write(sft_unstable_config)\n",
    "\n",
    "print(\"Created unstable SFT config with learning_rate=1.0e-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run unstable SFT\n",
    "!python -m canary.cli run configs/sft_unstable.yaml -o ./sft_output/unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stability metrics\n",
    "unstable_paths = list(Path('./sft_output/unstable').rglob('metrics.json'))\n",
    "\n",
    "if unstable_paths:\n",
    "    with open(unstable_paths[0]) as f:\n",
    "        unstable_metrics = json.load(f)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"UNSTABLE SFT ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    stability = unstable_metrics['stability']\n",
    "    print(f\"\\nStability Metrics:\")\n",
    "    print(f\"  NaN steps: {stability['nan_steps']} {'FAIL!' if stability['nan_steps'] > 0 else ''}\")\n",
    "    print(f\"  Inf steps: {stability['inf_steps']} {'FAIL!' if stability['inf_steps'] > 0 else ''}\")\n",
    "    print(f\"  Loss diverged: {stability['loss_diverged']} {'WARNING!' if stability['loss_diverged'] else ''}\")\n",
    "    print(f\"  Final loss: {stability['final_loss']}\")\n",
    "    \n",
    "    # Compare to baseline\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON TO BASELINE\")\n",
    "    print(\"=\"*60)\n",
    "    !python -m canary.cli compare {unstable_paths[0]} baselines/sft_baseline.json --threshold-tier smoke\n",
    "else:\n",
    "    print(\"Unstable run failed to produce metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When to Use SFT Canaries\n",
    "\n",
    "### SFT Canary Use Cases\n",
    "\n",
    "| Scenario | Why SFT Canary? |\n",
    "|----------|----------------|\n",
    "| **New model testing** | Quick validation before DPO/PPO |\n",
    "| **Infrastructure changes** | Fastest feedback on training pipeline |\n",
    "| **Memory-limited GPUs** | SFT fits when DPO/PPO won't |\n",
    "| **Baseline establishment** | Compare DPO/PPO improvements against SFT |\n",
    "| **CI/CD gating** | Fastest canary for PR validation |\n",
    "\n",
    "### SFT Canary Limitations\n",
    "\n",
    "- **No preference learning**: Can't test DPO-specific issues\n",
    "- **No RL dynamics**: Can't test PPO stability (KL, entropy, etc.)\n",
    "- **Simpler failure modes**: Fewer things can go wrong\n",
    "\n",
    "### Recommended Canary Strategy\n",
    "\n",
    "```\n",
    "PR Gating:     SFT smoke (5 min) → DPO smoke (10 min)\n",
    "Daily:         DPO perf (45 min) → PPO perf (60 min)\n",
    "Weekly:        Full suite with nightly configs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison of all training types\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL COMPARISON: SFT vs DPO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'SFT':>20} {'DPO':>20}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "print(f\"{'Training type':<25} {'sft':>20} {'dpo':>20}\")\n",
    "print(f\"{'Memory (MB)':<25} {sft_metrics['perf']['max_mem_mb']:>20.0f} {dpo_metrics['perf']['max_mem_mb']:>20.0f}\")\n",
    "print(f\"{'Step time (s)':<25} {sft_metrics['perf']['step_time']['mean']:>20.4f} {dpo_metrics['perf']['step_time']['mean']:>20.4f}\")\n",
    "print(f\"{'Tokens/sec':<25} {sft_metrics['perf']['approx_tokens_per_sec']:>20.0f} {dpo_metrics['perf']['approx_tokens_per_sec']:>20.0f}\")\n",
    "print(f\"{'Final loss':<25} {sft_metrics['stability']['final_loss']:>20.4f} {dpo_metrics['stability']['final_loss']:>20.4f}\")\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(f\"  - SFT uses {dpo_metrics['perf']['max_mem_mb']/sft_metrics['perf']['max_mem_mb']:.1f}x less memory than DPO\")\n",
    "print(f\"  - SFT is {sft_metrics['perf']['approx_tokens_per_sec']/dpo_metrics['perf']['approx_tokens_per_sec']:.1f}x faster (tokens/sec)\")\n",
    "print(f\"  - Use SFT for quick validation, DPO for preference learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Summary\n\n### Key Takeaways:\n\n1. **SFT is the simplest and fastest** training method\n2. **Memory efficient**: ~1x model vs DPO's ~2x and PPO's ~3x\n3. **Supports profiler**: Unlike PPO's manual loop\n4. **Same stability metrics**: NaN/Inf detection, loss divergence\n5. **Best for quick validation**: Before running expensive DPO/PPO\n\n### When to Choose SFT Canaries:\n\n- Infrastructure testing\n- PR gating (fastest feedback)\n- Memory-constrained environments\n- Baseline establishment\n\n### Related Notebooks:\n\n- `01_quickstart.ipynb` - DPO workflow basics\n- `05_ppo_canary.ipynb` - PPO with RL metrics\n- `02_profiler_deep_dive.ipynb` - Performance profiling\n- `03_stability_monitoring.ipynb` - Stability metrics"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}